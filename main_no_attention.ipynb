{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment 3 (RNN Frameworks for transliteration) - Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for the notebook\n",
    "import lightning as lt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from language import *\n",
    "from dataset_dataloader import *\n",
    "from encoder_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device to 'cuda' if available\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the source and target languages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source and target languages\n",
    "TARGET = 'hin'\n",
    "SOURCE = 'eng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n"
     ]
    }
   ],
   "source": [
    "# load all the available data and print sample counts for each set\n",
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 26\n",
      "Source Vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Source Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n",
      "Target Vocabulary Size = 64\n",
      "Target Vocabulary = ['ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्']\n",
      "Target Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'ँ', 5: 'ं', 6: 'ः', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ए', 15: 'ऐ', 16: 'ऑ', 17: 'ओ', 18: 'औ', 19: 'क', 20: 'ख', 21: 'ग', 22: 'घ', 23: 'ङ', 24: 'च', 25: 'छ', 26: 'ज', 27: 'झ', 28: 'ञ', 29: 'ट', 30: 'ठ', 31: 'ड', 32: 'ढ', 33: 'ण', 34: 'त', 35: 'थ', 36: 'द', 37: 'ध', 38: 'न', 39: 'प', 40: 'फ', 41: 'ब', 42: 'भ', 43: 'म', 44: 'य', 45: 'र', 46: 'ल', 47: 'ळ', 48: 'व', 49: 'श', 50: 'ष', 51: 'स', 52: 'ह', 53: '़', 54: 'ऽ', 55: 'ा', 56: 'ि', 57: 'ी', 58: 'ु', 59: 'ू', 60: 'ृ', 61: 'ॅ', 62: 'े', 63: 'ै', 64: 'ॉ', 65: 'ो', 66: 'ौ', 67: '्'}\n"
     ]
    }
   ],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "SRC_LANG = Language(SOURCE)\n",
    "TAR_LANG = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using train data only\n",
    "SRC_LANG.create_vocabulary(*(x_train))\n",
    "TAR_LANG.create_vocabulary(*(y_train))\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "SRC_LANG.generate_mappings()\n",
    "TAR_LANG.generate_mappings()\n",
    "\n",
    "# print the source and target vocabularies\n",
    "print(f'Source Vocabulary Size = {len(SRC_LANG.symbols)}')\n",
    "print(f'Source Vocabulary = {SRC_LANG.symbols}')\n",
    "print(f'Source Mapping {SRC_LANG.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(TAR_LANG.symbols)}')\n",
    "print(f'Target Vocabulary = {TAR_LANG.symbols}')\n",
    "print(f'Target Mapping {TAR_LANG.index2sym}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(lt.LightningModule):\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language, common_embed_size, common_num_layers, \n",
    "                 common_hidden_size, common_cell_type, enc_bidirect=False, attention=False, dropout=0.0, \n",
    "                 opt_name='Adam', learning_rate=1e-3, batch_size=32):\n",
    "    \n",
    "        super(Runner,self).__init__()\n",
    "        # save the language objects\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "\n",
    "        # create all the sub-networks and the main model\n",
    "        self.encoder = EncoderNet(vocab_size=src_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, bidirect=enc_bidirect, dropout=dropout)\n",
    "        if attention:\n",
    "            self.attention = True\n",
    "            self.attn_layer = Attention(common_hidden_size, enc_bidirect)\n",
    "        else:\n",
    "            self.attention = False\n",
    "            self.attn_layer = None\n",
    "        \n",
    "        self.decoder = DecoderNet(vocab_size=tar_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, attention=attention, attn_layer=self.attn_layer,\n",
    "                             enc_bidirect=enc_bidirect, dropout=dropout)\n",
    "        \n",
    "        self.model = EncoderDecoder(encoder=self.encoder, decoder=self.decoder, src_lang=src_lang, \n",
    "                                    tar_lang=tar_lang)\n",
    "\n",
    "        # for determinism\n",
    "        torch.manual_seed(42); torch.cuda.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "        self.model.apply(self.init_weights) # initialize model weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # optimizer for the model and loss function [that ignores locs where target = PAD token]\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(ignore_index=tar_lang.sym2index[PAD_SYM])\n",
    "        self.opt_name = opt_name\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # only adam is present in configure_optimizers as of now\n",
    "        if (opt_name != 'Adam'):\n",
    "            exit(-1)\n",
    "        \n",
    "        self.pred_train_words = []\n",
    "        self.true_train_words = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "        if self.opt_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        '''\n",
    "        function to initialize the weights of the model parameters\n",
    "        '''\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                 nn.init.uniform_(param.data, -0.04, 0.04)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_accuracy(pred_words, tar_words):\n",
    "        ''' \n",
    "        compute the accuracy using (predicted words, target words) and return it.\n",
    "        exact word matching is used.\n",
    "        '''\n",
    "        assert(len(pred_words) == len(tar_words))\n",
    "        count = 0\n",
    "        for i in range(len(pred_words)):\n",
    "            if pred_words[i] == tar_words[i]:\n",
    "                count += 1\n",
    "        return count / len(pred_words)\n",
    "    \n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # load all the available data on all GPUs\n",
    "        self.x_train, self.y_train = load_data(TARGET, 'train')\n",
    "        self.x_valid, self.y_valid = load_data(TARGET, 'valid')\n",
    "        self.x_test, self.y_test = load_data(TARGET, 'test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_train, self.y_train, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_valid, self.y_valid, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_test, self.y_test, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        # we do inference word by word. So, batch_size = 1\n",
    "        return dataloader\n",
    "\n",
    "    ####################\n",
    "    # INTERFACE RELATED FUNCTIONS - NOTE -> added validation and test methods; early stopping; heatmap; beam decoding;\n",
    "    #                                       save to file and plot test errors; fix teacher forcing time interval;\n",
    "    #                                       wandb sweeping stuff and model checkpointing; try TAMIL\n",
    "    ####################\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = train_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens, tf_ratio=0.8)\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        self.true_train_words.append(batch_y)\n",
    "        self.pred_train_words.append(preds)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        num_batches = len(self.pred_train_words)\n",
    "        pred, true = [], []\n",
    "        for i in range(num_batches):\n",
    "            pred += self.tar_lang.convert_to_words(self.pred_train_words[i])\n",
    "            true += self.tar_lang.convert_to_words(self.true_train_words[i])\n",
    "        print(len(true))\n",
    "        print(*zip(pred[:50],true[:50]), sep='\\n')\n",
    "        self.log('train_acc', 100*self.exact_accuracy(pred, true), on_epoch=True, prog_bar=True)\n",
    "        pred.clear(); true.clear()\n",
    "        self.pred_train_words.clear(); self.true_train_words.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 4.3 M \n",
      "1 | decoder        | DecoderNet       | 1.5 M \n",
      "2 | model          | EncoderDecoder   | 5.8 M \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.266    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007075309753417969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b66567dfac849428165a7a63d792dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51200\n",
      "('शस्त्रागरर', 'शस्त्रागार')\n",
      "('बिं्ध्य', 'बिन्द्या')\n",
      "('किरंकांत', 'किरणकांत')\n",
      "('यज्ञोपवित', 'यज्ञोपवीत')\n",
      "('रतैनिया', 'रटानिया')\n",
      "('वगगन्याचे', 'वागण्याचे')\n",
      "('देशभरामध्ये', 'देशभरामध्ये')\n",
      "('सुघा़पन', 'सुघड़पन')\n",
      "('मोहीवाल', 'मोहीवाल')\n",
      "('सर्वसंग्रह', 'सर्वसंग्रह')\n",
      "('बसेको', 'बसेको')\n",
      "('तुमच्यापैकी', 'तुमच्यापैकी')\n",
      "('कनन्यकुंभजम', 'कान्यकुब्ज')\n",
      "('इन्ोक्सिनेशन', 'इनटॉक्सिनेशन')\n",
      "('मैचुयूरिटी', 'मेच्यूरिटी')\n",
      "('अग्ी', 'अगरी')\n",
      "('अनुक्रमुनापात', 'अनुक्रमानुपात')\n",
      "('धुलचंदड', 'धूलचन्द')\n",
      "('अवलेहा', 'अवलेह')\n",
      "('अब्ोर्ड', 'एबरोर्ड')\n",
      "('बैलर्स', 'बैलर्स')\n",
      "('बावललीी', 'बार्गली')\n",
      "('पंक्डर्ड', 'पंक्चर्ड')\n",
      "('हैंकर्स', 'हैंकर्स')\n",
      "('जवानोंके', 'जवानोंके')\n",
      "('पैंकी', 'पौंकी')\n",
      "('जानधाम', 'जगनधाम')\n",
      "('पोनिनियम', 'पोन्नियम')\n",
      "('इईएनबीए', 'आईएनबीए')\n",
      "('वेदलम', 'वेदलम')\n",
      "('चिनवास', 'चिनवास')\n",
      "('मारवाड़ा', 'मारवाड़ा')\n",
      "('अन्मझा', 'अनसमझा')\n",
      "('इस्टूडेंट', 'इस्टूडेंट')\n",
      "('दुहखीत', 'दुःखीत')\n",
      "('सिघांची', 'सिघांची')\n",
      "('शिक्षेचे', 'शिक्षेचे')\n",
      "('उजववलतम', 'उज्वलतम')\n",
      "('अपत्टन', 'आपट्टन')\n",
      "('अम्बीकवन', 'अम्बिकावन')\n",
      "('खानी', 'खगनी')\n",
      "('थायामिन', 'थियामिन')\n",
      "('सोशियोलॉजिक', 'सोशियोलाजिकल')\n",
      "('ईपीपपी', 'एप')\n",
      "('भाजपकोो', 'भाजपाको')\n",
      "('इस्तिथी', 'इस्तिथि')\n",
      "('छायततंंड', 'छायाटांड')\n",
      "('टोपवाल', 'तोपवाल')\n",
      "('गुडवाते', 'गुदवाते')\n",
      "('विसलरीी', 'विसलेरी')\n",
      "51200\n",
      "('शस्त्रागार', 'शस्त्रागार')\n",
      "('बिंधद्या', 'बिन्द्या')\n",
      "('किरणकांत', 'किरणकांत')\n",
      "('यज्ञोपवीत', 'यज्ञोपवीत')\n",
      "('रतानिया', 'रटानिया')\n",
      "('वागण्याचे', 'वागण्याचे')\n",
      "('देशभरामध्ये', 'देशभरामध्ये')\n",
      "('सुघा़पन', 'सुघड़पन')\n",
      "('मोहीवाल', 'मोहीवाल')\n",
      "('सर्वसंग्रह', 'सर्वसंग्रह')\n",
      "('बसेको', 'बसेको')\n",
      "('तुमच्यापैकी', 'तुमच्यापैकी')\n",
      "('कान्यकुंगजा', 'कान्यकुब्ज')\n",
      "('इन्टस्सिकेशन', 'इनटॉक्सिनेशन')\n",
      "('मैच्यूरिटी', 'मेच्यूरिटी')\n",
      "('अगरी', 'अगरी')\n",
      "('अनुक्रमुनुपात', 'अनुक्रमानुपात')\n",
      "('धुलचं्द', 'धूलचन्द')\n",
      "('अवलेहा', 'अवलेह')\n",
      "('एब्रर्ड', 'एबरोर्ड')\n",
      "('बैलर्स', 'बैलर्स')\n",
      "('बावललीी', 'बार्गली')\n",
      "('पंक्चर्ड', 'पंक्चर्ड')\n",
      "('हैंकर्स', 'हैंकर्स')\n",
      "('जवानोंके', 'जवानोंके')\n",
      "('पैंकी', 'पौंकी')\n",
      "('जगनधाम', 'जगनधाम')\n",
      "('पोन्नियम', 'पोन्नियम')\n",
      "('आईएनबीए', 'आईएनबीए')\n",
      "('वेदलम', 'वेदलम')\n",
      "('चिनवास', 'चिनवास')\n",
      "('मारवाड़ा', 'मारवाड़ा')\n",
      "('अनसमझा', 'अनसमझा')\n",
      "('इस्टूडेंट', 'इस्टूडेंट')\n",
      "('दुभखीत', 'दुःखीत')\n",
      "('सिघांची', 'सिघांची')\n",
      "('शिक्षेचे', 'शिक्षेचे')\n",
      "('उज्वलतम', 'उज्वलतम')\n",
      "('आपट्टन', 'आपट्टन')\n",
      "('अम्बीकावन', 'अम्बिकावन')\n",
      "('खगनी', 'खगनी')\n",
      "('थियामिन', 'थियामिन')\n",
      "('सोशियोमॉजिकल', 'सोशियोलाजिकल')\n",
      "('ईपीपी', 'एप')\n",
      "('भाजपाको', 'भाजपाको')\n",
      "('इस्तिथि', 'इस्तिथि')\n",
      "('छायातंंड', 'छायाटांड')\n",
      "('तोपवाल', 'तोपवाल')\n",
      "('गुडवाते', 'गुदवाते')\n",
      "('विसलेरी', 'विसलेरी')\n",
      "51200\n",
      "('शस्त्रागार', 'शस्त्रागार')\n",
      "('बिं्ध्या', 'बिन्द्या')\n",
      "('किराकांत', 'किरणकांत')\n",
      "('यज्ञोपवीत', 'यज्ञोपवीत')\n",
      "('रतानिया', 'रटानिया')\n",
      "('वागण्याचे', 'वागण्याचे')\n",
      "('देशभरामध्ये', 'देशभरामध्ये')\n",
      "('सुघा़पन', 'सुघड़पन')\n",
      "('मोहीवाल', 'मोहीवाल')\n",
      "('सर्वसंग्रह', 'सर्वसंग्रह')\n",
      "('बसेको', 'बसेको')\n",
      "('तुमच्यापैकी', 'तुमच्यापैकी')\n",
      "('काय्यकुं्ज', 'कान्यकुब्ज')\n",
      "('इंटोक्सिनेशन', 'इनटॉक्सिनेशन')\n",
      "('मैच्यूरिटी', 'मेच्यूरिटी')\n",
      "('अगरी', 'अगरी')\n",
      "('अनुक्रमानुपात', 'अनुक्रमानुपात')\n",
      "('धुपचं्द', 'धूलचन्द')\n",
      "('अवलेह', 'अवलेह')\n",
      "('एब्ोर्ड', 'एबरोर्ड')\n",
      "('बैलर्स', 'बैलर्स')\n",
      "('बावीलीी', 'बार्गली')\n",
      "('पंच्चर्ड', 'पंक्चर्ड')\n",
      "('हैंकर्स', 'हैंकर्स')\n",
      "('जवानोंके', 'जवानोंके')\n",
      "('पैंकी', 'पौंकी')\n",
      "('जगनधाम', 'जगनधाम')\n",
      "('पोन्नियम', 'पोन्नियम')\n",
      "('आईएनबीए', 'आईएनबीए')\n",
      "('वेदलम', 'वेदलम')\n",
      "('चिनवास', 'चिनवास')\n",
      "('मारवाड़ा', 'मारवाड़ा')\n",
      "('अनसमझा', 'अनसमझा')\n",
      "('इस्टूडेंट', 'इस्टूडेंट')\n",
      "('दुभखीत', 'दुःखीत')\n",
      "('सिघांची', 'सिघांची')\n",
      "('शिक्षेचे', 'शिक्षेचे')\n",
      "('उज्वलतम', 'उज्वलतम')\n",
      "('आपट्टन', 'आपट्टन')\n",
      "('अम्बीकावन', 'अम्बिकावन')\n",
      "('खगनी', 'खगनी')\n",
      "('थियामिन', 'थियामिन')\n",
      "('सोशियोलॉजिकल', 'सोशियोलाजिकल')\n",
      "('एपीपप', 'एप')\n",
      "('भाजपाको', 'भाजपाको')\n",
      "('इस्तिथि', 'इस्तिथि')\n",
      "('छायततंंड', 'छायाटांड')\n",
      "('तोपवाल', 'तोपवाल')\n",
      "('गुदवाते', 'गुदवाते')\n",
      "('विसरेरी', 'विसलेरी')\n",
      "51200\n",
      "('शस्त्रागार', 'शस्त्रागार')\n",
      "('बिं्द्या', 'बिन्द्या')\n",
      "('किरणकांत', 'किरणकांत')\n",
      "('यज्ञोपवीत', 'यज्ञोपवीत')\n",
      "('रतानिया', 'रटानिया')\n",
      "('वागण्याचे', 'वागण्याचे')\n",
      "('देशभरामध्ये', 'देशभरामध्ये')\n",
      "('सुघड़पन', 'सुघड़पन')\n",
      "('मोहीवाल', 'मोहीवाल')\n",
      "('सर्वसंग्रह', 'सर्वसंग्रह')\n",
      "('बसेको', 'बसेको')\n",
      "('तुमच्यापैकी', 'तुमच्यापैकी')\n",
      "('कान्यकुं्ज', 'कान्यकुब्ज')\n",
      "('इं्ाक्सिनेशन', 'इनटॉक्सिनेशन')\n",
      "('मैच्यूरिटी', 'मेच्यूरिटी')\n",
      "('अगरी', 'अगरी')\n",
      "('अनुक्रमानुपात', 'अनुक्रमानुपात')\n",
      "('ढूलचन्द', 'धूलचन्द')\n",
      "('अवलेहा', 'अवलेह')\n",
      "('एबरोर्ड', 'एबरोर्ड')\n",
      "('बैलररस', 'बैलर्स')\n",
      "('बावलीी', 'बार्गली')\n",
      "('पंक्चर्ड', 'पंक्चर्ड')\n",
      "('हैंकर्स', 'हैंकर्स')\n",
      "('जवानोंके', 'जवानोंके')\n",
      "('पैंकी', 'पौंकी')\n",
      "('जगनधाम', 'जगनधाम')\n",
      "('पोन्नियम', 'पोन्नियम')\n",
      "('इईएनबीए', 'आईएनबीए')\n",
      "('वेदलम', 'वेदलम')\n",
      "('चिनवास', 'चिनवास')\n",
      "('मारवाड़ा', 'मारवाड़ा')\n",
      "('अनसमझा', 'अनसमझा')\n",
      "('इस्टूडेंट', 'इस्टूडेंट')\n",
      "('दुहखीत', 'दुःखीत')\n",
      "('सिघांची', 'सिघांची')\n",
      "('शिक्षेचे', 'शिक्षेचे')\n",
      "('उज्वलतम', 'उज्वलतम')\n",
      "('आपट्टान', 'आपट्टन')\n",
      "('अम्बीकावन', 'अम्बिकावन')\n",
      "('खगनी', 'खगनी')\n",
      "('थायामाइ', 'थियामिन')\n",
      "('सोशियोलाजिकल', 'सोशियोलाजिकल')\n",
      "('एपीपी', 'एप')\n",
      "('भाजपाको', 'भाजपाको')\n",
      "('इस्तिथि', 'इस्तिथि')\n",
      "('छायातांड', 'छायाटांड')\n",
      "('टोपवाल', 'तोपवाल')\n",
      "('गुडवाते', 'गुदवाते')\n",
      "('विसलेरी', 'विसलेरी')\n",
      "51200\n",
      "('शस्त्रामार', 'शस्त्रागार')\n",
      "('बिं्द्या', 'बिन्द्या')\n",
      "('किरणकांत', 'किरणकांत')\n",
      "('यज्ञोपवीत', 'यज्ञोपवीत')\n",
      "('रतानिया', 'रटानिया')\n",
      "('वागण्याचे', 'वागण्याचे')\n",
      "('देशभरामध्ये', 'देशभरामध्ये')\n",
      "('सुघा़पन', 'सुघड़पन')\n",
      "('मोहीवाल', 'मोहीवाल')\n",
      "('सर्वसंग्रह', 'सर्वसंग्रह')\n",
      "('बसेको', 'बसेको')\n",
      "('तुमच्यापैकी', 'तुमच्यापैकी')\n",
      "('कनन्यकुम्ज', 'कान्यकुब्ज')\n",
      "('इनटोक्लियेशन', 'इनटॉक्सिनेशन')\n",
      "('मैच्यूरिटी', 'मेच्यूरिटी')\n",
      "('अगरी', 'अगरी')\n",
      "('अनुक्रमानुपतत', 'अनुक्रमानुपात')\n",
      "('धूलचन्द', 'धूलचन्द')\n",
      "('अवलेह', 'अवलेह')\n",
      "('एबरोर्ड', 'एबरोर्ड')\n",
      "('बैलर्स', 'बैलर्स')\n",
      "('बववली', 'बार्गली')\n",
      "('पंक्चर्ड', 'पंक्चर्ड')\n",
      "('हैंकर्स', 'हैंकर्स')\n",
      "('जवानोंके', 'जवानोंके')\n",
      "('पैंकी', 'पौंकी')\n",
      "('जगन्म', 'जगनधाम')\n",
      "('पोन्नियम', 'पोन्नियम')\n",
      "('आईएनबीए', 'आईएनबीए')\n",
      "('वेदलम', 'वेदलम')\n",
      "('चिनवास', 'चिनवास')\n",
      "('मारवाड़ा', 'मारवाड़ा')\n",
      "('अनसमझा', 'अनसमझा')\n",
      "('इस्टूडेंट', 'इस्टूडेंट')\n",
      "('दुहखीत', 'दुःखीत')\n",
      "('सिघांची', 'सिघांची')\n",
      "('शिक्षेचे', 'शिक्षेचे')\n",
      "('उज्वलतम', 'उज्वलतम')\n",
      "('आपत्टन', 'आपट्टन')\n",
      "('अम्बीकावन', 'अम्बिकावन')\n",
      "('खगनी', 'खगनी')\n",
      "('थियामिन', 'थियामिन')\n",
      "('सोशियोलॉजिकल', 'सोशियोलाजिकल')\n",
      "('एपीपपी', 'एप')\n",
      "('भाजपाको', 'भाजपाको')\n",
      "('इस्तिथि', 'इस्तिथि')\n",
      "('छायातंंड', 'छायाटांड')\n",
      "('तोपवाल', 'तोपवाल')\n",
      "('गुदवाते', 'गुदवाते')\n",
      "('विसलेरी', 'विसलेरी')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "#runner = Runner(SRC_LANG, TAR_LANG, 128, 3, 256, 'LSTM', True, False, 0.0, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=5)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 199 K \n",
      "1 | decoder        | DecoderNet       | 93.9 K\n",
      "2 | model          | EncoderDecoder   | 293 K \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "293 K     Trainable params\n",
      "0         Non-trainable params\n",
      "293 K     Total params\n",
      "1.175     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008435487747192383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e8d8635c8243f6af3dbf2a75a76de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 1, 128, 'LSTM', True, False, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 158 K \n",
      "1 | decoder        | DecoderNet       | 73.2 K\n",
      "2 | model          | EncoderDecoder   | 231 K \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "231 K     Trainable params\n",
      "0         Non-trainable params\n",
      "231 K     Total params\n",
      "0.926     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005796194076538086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9e3f04d98d4d5b8e92fa45ce9b0aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 1, 128, 'GRU', True, False, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 158 K \n",
      "1 | attn_layer     | Attention        | 49.4 K\n",
      "2 | decoder        | DecoderNet       | 220 K \n",
      "3 | model          | EncoderDecoder   | 379 K \n",
      "4 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "379 K     Trainable params\n",
      "0         Non-trainable params\n",
      "379 K     Total params\n",
      "1.517     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006476879119873047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c212e0fa77f24320a408fe3afe515cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 1, 128, 'GRU', True, True, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 1.1 M \n",
      "1 | decoder        | DecoderNet       | 358 K \n",
      "2 | model          | EncoderDecoder   | 1.4 M \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.657     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0061833858489990234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7315b133f94f491bb1b296065eff0064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 3, 128, 'LSTM', True, False, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 1.1 M \n",
      "1 | decoder        | DecoderNet       | 358 K \n",
      "2 | model          | EncoderDecoder   | 1.4 M \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.657     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008263587951660156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da42ccbf1f1c4648b37d861190e90f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 3, 128, 'LSTM', True, False, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 816 K \n",
      "1 | decoder        | DecoderNet       | 271 K \n",
      "2 | model          | EncoderDecoder   | 1.1 M \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.353     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0064508914947509766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2c04d28cc441a2b4a3445da7cfa1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 3, 128, 'GRU', True, False, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 816 K \n",
      "1 | attn_layer     | Attention        | 49.4 K\n",
      "2 | decoder        | DecoderNet       | 419 K \n",
      "3 | model          | EncoderDecoder   | 1.2 M \n",
      "4 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.944     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0074310302734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6862d7d7529463eaf84df4f6174c077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# testing runner\n",
    "# keep embedding small (around 32) -> important to get dense embedding\n",
    "# also, adjust learning rate reasonably\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 32, 3, 128, 'GRU', True, True, 0.2, 'Adam', learning_rate=1e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=1)\n",
    "trainer.fit(runner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
