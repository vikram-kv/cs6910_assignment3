{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for the notebook\n",
    "import lightning as lt\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from torchaudio.functional import edit_distance as edit_dist\n",
    "import random\n",
    "from language import *\n",
    "from dataset_dataloader import *\n",
    "from encoder_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# know the accelerator available - NOT USED as we have switched to lightning\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the source and target languages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source and target languages\n",
    "TARGET = 'tam'\n",
    "SOURCE = 'eng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n"
     ]
    }
   ],
   "source": [
    "# load all the available data and print sample counts for each set\n",
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 26\n",
      "Source Vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Source Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n",
      "Target Vocabulary Size = 46\n",
      "Target Vocabulary = ['ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்']\n",
      "Target Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'ஃ', 5: 'அ', 6: 'ஆ', 7: 'இ', 8: 'ஈ', 9: 'உ', 10: 'ஊ', 11: 'எ', 12: 'ஏ', 13: 'ஐ', 14: 'ஒ', 15: 'ஓ', 16: 'க', 17: 'ங', 18: 'ச', 19: 'ஜ', 20: 'ஞ', 21: 'ட', 22: 'ண', 23: 'த', 24: 'ந', 25: 'ன', 26: 'ப', 27: 'ம', 28: 'ய', 29: 'ர', 30: 'ற', 31: 'ல', 32: 'ள', 33: 'ழ', 34: 'வ', 35: 'ஷ', 36: 'ஸ', 37: 'ஹ', 38: 'ா', 39: 'ி', 40: 'ீ', 41: 'ு', 42: 'ூ', 43: 'ெ', 44: 'ே', 45: 'ை', 46: 'ொ', 47: 'ோ', 48: 'ௌ', 49: '்'}\n"
     ]
    }
   ],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "SRC_LANG = Language(SOURCE)\n",
    "TAR_LANG = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using train data only\n",
    "SRC_LANG.create_vocabulary(*(x_train))\n",
    "TAR_LANG.create_vocabulary(*(y_train))\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "SRC_LANG.generate_mappings()\n",
    "TAR_LANG.generate_mappings()\n",
    "\n",
    "# print the source and target vocabularies\n",
    "print(f'Source Vocabulary Size = {len(SRC_LANG.symbols)}')\n",
    "print(f'Source Vocabulary = {SRC_LANG.symbols}')\n",
    "print(f'Source Mapping {SRC_LANG.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(TAR_LANG.symbols)}')\n",
    "print(f'Target Vocabulary = {TAR_LANG.symbols}')\n",
    "print(f'Target Mapping {TAR_LANG.index2sym}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(lt.LightningModule):\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language, common_embed_size, common_num_layers, \n",
    "                 common_hidden_size, common_cell_type, init_tf_ratio = 0.8, enc_bidirect=False, attention=False, dropout=0.0, \n",
    "                 opt_name='Adam', learning_rate=2e-3, batch_size=32):\n",
    "    \n",
    "        super(Runner,self).__init__()\n",
    "        # save the language objects\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "\n",
    "        # create all the sub-networks and the main model\n",
    "        self.encoder = EncoderNet(vocab_size=src_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, bidirect=enc_bidirect, dropout=dropout)\n",
    "        if attention:\n",
    "            self.attention = True\n",
    "            self.attn_layer = Attention(common_hidden_size, enc_bidirect)\n",
    "        else:\n",
    "            self.attention = False\n",
    "            self.attn_layer = None\n",
    "        \n",
    "        self.decoder = DecoderNet(vocab_size=tar_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, attention=attention, attn_layer=self.attn_layer,\n",
    "                             enc_bidirect=enc_bidirect, dropout=dropout)\n",
    "        \n",
    "        self.model = EncoderDecoder(encoder=self.encoder, decoder=self.decoder, src_lang=src_lang, \n",
    "                                    tar_lang=tar_lang)\n",
    "\n",
    "        # for determinism\n",
    "        torch.manual_seed(42); torch.cuda.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "        self.model.apply(self.init_weights) # initialize model weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # optimizer for the model and loss function [that ignores locs where target = PAD token]\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(ignore_index=tar_lang.sym2index[PAD_SYM])\n",
    "        self.opt_name = opt_name\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # only adam is present in configure_optimizers as of now\n",
    "        if (opt_name != 'Adam'):\n",
    "            exit(-1)\n",
    "        \n",
    "        self.pred_train_words = []; self.true_train_words = []\n",
    "        self.pred_valid_words = []; self.true_valid_words = []\n",
    "        self.test_X_words = []; self.pred_test_words = []; self.true_test_words = []\n",
    "        self.save_test_preds = False\n",
    "        self.cur_tf_ratio = init_tf_ratio\n",
    "        self.min_tf_ratio = 0.01\n",
    "        self.attn_matrices = []  # used only when there is attention layer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "        if self.opt_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        '''\n",
    "        function to initialize the weights of the model parameters\n",
    "        '''\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                 nn.init.uniform_(param.data, -0.04, 0.04)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_accuracy(pred_words, tar_words):\n",
    "        ''' \n",
    "        compute the accuracy using (predicted words, target words) and return it.\n",
    "        exact word matching is used.\n",
    "        '''\n",
    "        assert(len(pred_words) == len(tar_words))\n",
    "        count = 0\n",
    "        for i in range(len(pred_words)):\n",
    "            if pred_words[i] == tar_words[i]:\n",
    "                count += 1\n",
    "        return count / len(pred_words)\n",
    "    \n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # load all the available data on all GPUs\n",
    "        self.x_train, self.y_train = load_data(TARGET, 'train')\n",
    "        self.x_valid, self.y_valid = load_data(TARGET, 'valid')\n",
    "        self.x_test, self.y_test = load_data(TARGET, 'test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_train, self.y_train, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_valid, self.y_valid, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_test, self.y_test, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        # we do inference word by word. So, batch_size = 1\n",
    "        return dataloader\n",
    "\n",
    "    ####################\n",
    "    # INTERFACE RELATED FUNCTIONS - NOTE -> heatmap; beam decoding (and save top 3 preds);\n",
    "    #                                       \n",
    "    #                                       wandb sweeping stuff and model checkpointing;\n",
    "    #                                       put all together\n",
    "    ####################\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = train_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens, tf_ratio=self.cur_tf_ratio)\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        # for epoch-level metrics[accuracy], log all the required data\n",
    "        self.true_train_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.pred_train_words += self.tar_lang.convert_to_words(preds)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_accuracy', self.exact_accuracy(self.pred_train_words, self.true_train_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.pred_train_words.clear(); self.true_train_words.clear()\n",
    "\n",
    "        self.log('tf_ratio', self.cur_tf_ratio, on_epoch=True, prog_bar=True)\n",
    "        # for first 12 epochs, we dont change the tf ratio. Then we decrease it by 0.1 every epoch till\n",
    "        # min_tf_ratio is reached. This is also logged.\n",
    "        if (self.current_epoch >= 11):\n",
    "            self.cur_tf_ratio -= 0.1\n",
    "            self.cur_tf_ratio = max(self.cur_tf_ratio, self.min_tf_ratio)\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = valid_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens) # no teacher forcing\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        # for epoch-level metrics[accuracy], log all the required data\n",
    "        self.true_valid_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.pred_valid_words += self.tar_lang.convert_to_words(preds)\n",
    "        self.log('validation_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log('validation_accuracy', self.exact_accuracy(self.true_valid_words, self.pred_valid_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.true_valid_words.clear(); self.pred_valid_words.clear()\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = test_batch\n",
    "        logits, pred_word, attn_matrix = self.model.greedy_inference(batch_X, X_lens)\n",
    "        # update all the global lists\n",
    "        self.pred_test_words += pred_word\n",
    "        self.true_test_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.test_X_words += self.src_lang.convert_to_words(batch_X)\n",
    "        # if there is attention, update the attention list also\n",
    "        if (self.attention):\n",
    "            self.attn_matrices += [attn_matrix]\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[1:, :]\n",
    "        # we shrink the logits to the true decoded sequence length for loss computation alone\n",
    "        true_dec_len = targets.size(1)\n",
    "        logits = (logits[:true_dec_len, :]).swapaxes(0,1).unsqueeze(0)\n",
    "        # squeeze and swapping of dimensions is to meet condition needed by nn.CrossEntopyLoss()\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    # will prevent clearing of global test lists on test epoch end\n",
    "    def track_test_predictions(self):\n",
    "        self.save_test_preds = True\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log('test_accuracy', self.exact_accuracy(self.pred_test_words, self.true_test_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        if not self.save_test_preds:\n",
    "            self.pred_test_words.clear(); self.true_test_words.clear(); self.test_X_words.clear()\n",
    "            self.attn_matrices.clear()\n",
    "    \n",
    "    # here, we will save all the predictions made and also, return a copy of the list of attention\n",
    "    # matrices for generating heatmaps\n",
    "    def save_test_predictions(self, fname='pred'):\n",
    "        edit_distances = [edit_dist(pred,tar) for pred, tar in zip(self.pred_test_words,self.true_test_words)]\n",
    "        pred_df = pd.DataFrame(zip(self.test_X_words, self.true_test_words, self.pred_test_words, edit_distances),\n",
    "                               columns=['Input', 'Target', 'Predicted', 'Levenshtein Distance'])\n",
    "        pred_df.to_csv('./'+fname+'.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        # if attention layer is present, we return attention matrices as well.\n",
    "        ret_info = None\n",
    "        if self.attention:\n",
    "            ret_info = (self.test_X_words.copy(), self.pred_test_words.copy(), self.attn_matrices.copy())\n",
    "        self.save_test_preds = False\n",
    "        # clear after saving to save memory \n",
    "        self.pred_test_words.clear(); self.true_test_words.clear(); self.test_X_words.clear()\n",
    "        self.attn_matrices.clear()\n",
    "        return ret_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 4.3 M \n",
      "1 | attn_layer     | Attention        | 197 K \n",
      "2 | decoder        | DecoderNet       | 2.2 M \n",
      "3 | model          | EncoderDecoder   | 6.5 M \n",
      "4 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "26.123    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.024852275848388672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccd9a7d947a4a9499c5a740f3ac831f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikram/.local/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/vikram/.local/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008487462997436523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03ad2bc902c4c268e6872390aa897fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=20` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/vikram/.local/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005959987640380859,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bd16c1910a48e898aa8c4a9b7ce099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing runner\n",
    "# send tf_ratio (hparam) for 10 epochs (min_epochs); then turn on early stopping to track val_loss/val_acc\n",
    "# BEST CONFIG - NOTE (attn = True, 12 epochs, tf_ratio=0.8)\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 128, 3, 256, 'LSTM', 0.8, True, True, 0.0, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_steps=20)\n",
    "\n",
    "trainer.fit(runner)\n",
    "runner.freeze()\n",
    "trainer.test(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'no-attention-bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'validation_accuracy'},\n",
    "    'parameters': {\n",
    "        'embedding_size' : {'values' : [16, 32, 64, 128, 192]},\n",
    "        'number_of_layers' : {'values' : [1, 2, 3]},\n",
    "        'hidden_size' : {'values' : [32, 64, 128, 192, 256]},\n",
    "        'cell' : {'values' : ['RNN', 'GRU', 'LSTM']},\n",
    "        'bidirectional' : {'values' : ['True', 'False']},\n",
    "        'dropout' : {'values' : [0.0, 0.05, 0.1, 0.2, 0.3]},\n",
    "        'initial_tf_ratio' : {'values' : [0.6, 0.7, 0.8, 0.9]},\n",
    "        'batch_size' : {'values' : [32, 64, 128]},\n",
    "        'attention' : {'value' : 'False'},\n",
    "        'dataset' : {'value' : 'aksharantar'}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, entity='cs19b021', project='cs6910-assignment3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing runner\n",
    "# send tf_ratio (hparam) for 10 epochs (min_epochs); then turn on early stopping to track val_loss/val_acc\n",
    "# BEST CONFIG - NOTE (attn = True, 12 epochs, tf_ratio=0.8)\n",
    "\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 4, 1, 4, 'LSTM', True, False, 0.0, 'Adam', learning_rate=2e-3, batch_size=64)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_accuracy\", min_delta=0.1, patience=5, verbose=True, mode=\"max\")\n",
    "tblogger = TensorBoardLogger(os.getcwd())\n",
    "wdblogger = WandbLogger(log_model='all')\n",
    "chkpt_callback = ModelCheckpoint(monitor='validation_accuracy', mode='max')\n",
    "trainer = lt.Trainer(min_epochs=10, max_epochs=35, callbacks=[chkpt_callback, early_stop_callback], logger=[tblogger, WandbLogger])\n",
    "\n",
    "trainer.fit(runner)\n",
    "runner.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE -> performing BUG SEARCH\n",
    "# testing all combinations to catch bugs\n",
    "# RESULT -> all clear; no bugs caught\n",
    "num_lay = [1,3]\n",
    "ctype = ['LSTM', 'GRU', 'RNN']\n",
    "bidirect = [True, False]\n",
    "attn = [True, False]\n",
    "\n",
    "for n in num_lay:\n",
    "    for c in ctype:\n",
    "        for b in bidirect:\n",
    "            for a in attn:\n",
    "                runner = Runner(SRC_LANG, TAR_LANG, 128, n, 256, c, b, a, 0.05, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "                trainer = lt.Trainer(max_steps=50)\n",
    "                trainer.fit(runner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
