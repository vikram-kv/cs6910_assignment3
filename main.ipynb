{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment 3 (RNN Frameworks for transliteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "LANG = 'tam'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the 'cat' (= train/val/test) data of language 'lang'\n",
    "def load_data(lang, cat):\n",
    "    fcontents = open(f'aksharantar_sampled/{lang}/{lang}_{cat}.csv','r', encoding='utf-8').readlines()\n",
    "    pairs = [tuple(l.strip().split(',')) for l in fcontents]\n",
    "    x_data, y_data = list(map(list,zip(*pairs)))\n",
    "    return x_data, y_data\n",
    "\n",
    "# function to create the vocabulary using the words in 'data'\n",
    "def create_vocabulary(*data):\n",
    "    symbols = set()\n",
    "    for wd in data:\n",
    "        for c in wd:\n",
    "            symbols.add(c)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_data(LANG, 'train')\n",
    "x_valid, y_valid = load_data(LANG, 'valid')\n",
    "x_test, y_test = load_data(LANG, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 26\n",
      "Source Vocabulary = {'s', 'o', 'y', 'u', 'c', 'h', 'l', 't', 'n', 'f', 'a', 'r', 'm', 'k', 'p', 'v', 'd', 'g', 'x', 'q', 'b', 'j', 'z', 'i', 'e', 'w'};\n",
      "Target Vocabulary Size = 47\n",
      "Target Vocabulary = {'ீ', 'எ', 'ெ', 'த', 'ஞ', 'ஷ', 'ி', 'ச', 'அ', 'ள', 'ந', 'ஏ', 'க', 'ஂ', 'ூ', 'ல', 'ஓ', 'ொ', 'ோ', 'ே', 'ஜ', 'ங', 'ஊ', 'ஆ', 'ு', 'இ', 'ட', 'ஃ', 'ஐ', 'ம', 'உ', 'ய', 'ண', 'ற', 'ஒ', 'ழ', 'ஸ', 'ப', 'ர', 'ன', 'வ', 'ஹ', 'ா', 'ஈ', 'ை', 'ௌ', '்'};\n"
     ]
    }
   ],
   "source": [
    "eng_vocab = create_vocabulary(*(x_train), *(x_valid), *(x_test))\n",
    "lang_vocab = create_vocabulary(*(y_train), *(y_valid), *(y_test))\n",
    "\n",
    "print(f'Source Vocabulary Size = {len(eng_vocab)}')\n",
    "print(f'Source Vocabulary = {eng_vocab};')\n",
    "print(f'Target Vocabulary Size = {len(lang_vocab)}')\n",
    "print(f'Target Vocabulary = {lang_vocab};')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
