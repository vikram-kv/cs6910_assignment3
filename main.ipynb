{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment 3 (RNN Frameworks for transliteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import wandb\n",
    "import unicodedata\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "TARGET = 'tam'\n",
    "SOURCE = 'eng'\n",
    "SOS_SYM = '@'\n",
    "EOS_SYM = '$'\n",
    "UNK_SYM = '!'\n",
    "PAD_SYM = '%'\n",
    "\n",
    "unicode_ranges = {'tam' : [0x0B80, 0x0BFF], \n",
    "                  'eng' : [0x0061, 0x007A],\n",
    "                  'hin' : [0x0900, 0x097F]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the 'cat' (= train/val/test) data of language 'lang'\n",
    "def load_data(lang, cat):\n",
    "    fcontents = open(f'aksharantar_sampled/{lang}/{lang}_{cat}.csv','r', encoding='utf-8').readlines()\n",
    "    pairs = [tuple(l.strip().split(',')) for l in fcontents]\n",
    "    x_data, y_data = list(map(list,zip(*pairs)))\n",
    "    return x_data, y_data\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.lname = name\n",
    "    \n",
    "    # function to create the vocabulary using the words in 'data'\n",
    "    def create_vocabulary(self, *data):\n",
    "        symbols = set()\n",
    "        for wd in data:\n",
    "            for c in wd:\n",
    "                symbols.add(c)\n",
    "        self.symbols = symbols\n",
    "\n",
    "    # function to use unicode ranges for creating the character set\n",
    "    def create_vocabulary_range(self):\n",
    "        symbols = set()\n",
    "        begin, end = unicode_ranges[self.lname]\n",
    "        for i in range(begin, end+1):\n",
    "            if (unicodedata.category(chr(i)) != 'Cn'):\n",
    "                symbols.add(chr(i))\n",
    "        self.symbols = symbols\n",
    "    \n",
    "    def generate_mappings(self):\n",
    "        self.index2sym = {0: SOS_SYM, 1 : EOS_SYM, 2 : UNK_SYM, 3 : PAD_SYM}\n",
    "        self.sym2index = {SOS_SYM : 0, EOS_SYM : 1, UNK_SYM : 2, PAD_SYM : 3}\n",
    "        self.symbols = list(self.symbols)\n",
    "        self.symbols.sort()\n",
    "\n",
    "        for i, sym in enumerate(self.symbols):\n",
    "            self.sym2index[sym] = i + 3\n",
    "            self.index2sym[i+3] = sym\n",
    "        \n",
    "        self.num_tokens = len(self.index2sym.keys())\n",
    "    \n",
    "    def convert_to_numbers(self, word):\n",
    "        enc = [self.sym2index[SOS_SYM]]\n",
    "        for ch in word:\n",
    "            if ch in self.sym2index.keys():\n",
    "                enc.append(self.sym2index[ch])\n",
    "            else:\n",
    "                enc.append(self.sym2index[UNK_SYM])\n",
    "        enc.append(self.sym2index[EOS_SYM])\n",
    "        return enc\n",
    "\n",
    "    def get_index(self, sym):\n",
    "        return self.sym2index[sym]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 26\n",
      "Source Vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Source Mapping {0: '@', 1: '$', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n",
      "Target Vocabulary Size = 72\n",
      "Target Vocabulary = ['ஂ', 'ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'ஔ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஶ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்', 'ௐ', 'ௗ', '௦', '௧', '௨', '௩', '௪', '௫', '௬', '௭', '௮', '௯', '௰', '௱', '௲', '௳', '௴', '௵', '௶', '௷', '௸', '௹', '௺']\n",
      "Target Mapping {0: '@', 1: '$', 2: 'ஂ', 3: 'ஃ', 4: 'அ', 5: 'ஆ', 6: 'இ', 7: 'ஈ', 8: 'உ', 9: 'ஊ', 10: 'எ', 11: 'ஏ', 12: 'ஐ', 13: 'ஒ', 14: 'ஓ', 15: 'ஔ', 16: 'க', 17: 'ங', 18: 'ச', 19: 'ஜ', 20: 'ஞ', 21: 'ட', 22: 'ண', 23: 'த', 24: 'ந', 25: 'ன', 26: 'ப', 27: 'ம', 28: 'ய', 29: 'ர', 30: 'ற', 31: 'ல', 32: 'ள', 33: 'ழ', 34: 'வ', 35: 'ஶ', 36: 'ஷ', 37: 'ஸ', 38: 'ஹ', 39: 'ா', 40: 'ி', 41: 'ீ', 42: 'ு', 43: 'ூ', 44: 'ெ', 45: 'ே', 46: 'ை', 47: 'ொ', 48: 'ோ', 49: 'ௌ', 50: '்', 51: 'ௐ', 52: 'ௗ', 53: '௦', 54: '௧', 55: '௨', 56: '௩', 57: '௪', 58: '௫', 59: '௬', 60: '௭', 61: '௮', 62: '௯', 63: '௰', 64: '௱', 65: '௲', 66: '௳', 67: '௴', 68: '௵', 69: '௶', 70: '௷', 71: '௸', 72: '௹', 73: '௺'}\n"
     ]
    }
   ],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "src_lang = Language(SOURCE)\n",
    "tar_lang = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using all data\n",
    "src_lang.create_vocabulary(*(x_train), *(x_valid), *(x_test))\n",
    "tar_lang.create_vocabulary(*(y_train), *(y_valid), *(y_test))\n",
    "\n",
    "# otherwise, use unicode characters (assigned codepoints) in the script's range\n",
    "# src_lang.create_vocabulary_range()\n",
    "# tar_lang.create_vocabulary_range()\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "src_lang.generate_mappings()\n",
    "tar_lang.generate_mappings()\n",
    "\n",
    "print(f'Source Vocabulary Size = {len(src_lang.symbols)}')\n",
    "print(f'Source Vocabulary = {src_lang.symbols}')\n",
    "print(f'Source Mapping {src_lang.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(tar_lang.symbols)}')\n",
    "print(f'Target Vocabulary = {tar_lang.symbols}')\n",
    "print(f'Target Mapping {tar_lang.index2sym}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterateDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, src_lang : Language, tar_lang : Language):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.x_data[idx], self.y_data[idx]\n",
    "        x = self.src_lang.convert_to_numbers(x)\n",
    "        y = self.tar_lang.convert_to_numbers(y) \n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "\n",
    "class CollationFunction:\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language):\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        src, tar = zip(*batch)\n",
    "        src = pad_sequence(list(src), batch_first=True, padding_value=src_lang.get_index(PAD_SYM))\n",
    "        tar = pad_sequence(list(tar), batch_first=True, padding_value=tar_lang.get_index(PAD_SYM))\n",
    "        return src, tar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, hid_size, cell_type, \n",
    "                 bidirect=False, dropout=0):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.hidden_size = hid_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # we create the required architecture using the received parameters\n",
    "        if cell_type == 'RNN':\n",
    "            self.network = nn.RNN(input_size=embed_size, hidden_size=hid_size, num_layers=num_layers, \n",
    "                               dropout=dropout, bidirectional=bidirect)\n",
    "        elif cell_type == 'LSTM':\n",
    "            self.network = nn.LSTM(input_size=embed_size, hidden_size=hid_size, num_layers=num_layers, \n",
    "                               dropout=dropout, bidirectional=bidirect)\n",
    "        else:\n",
    "            self.network = nn.GRU(input_size=embed_size, hidden_size=hid_size, num_layers=num_layers, \n",
    "                               dropout=dropout, bidirectional=bidirect)\n",
    "        \n",
    "        self.cell_type = cell_type\n",
    "        self.bidirect = bidirect\n",
    "        \n",
    "    def forward():\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
