{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment 3 (RNN Frameworks for transliteration) - Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for the notebook\n",
    "import pytorch_lightning as plt\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import display\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.functional import edit_distance as edit_dist\n",
    "import random\n",
    "from language import *\n",
    "from dataset_dataloader import *\n",
    "from encoder_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# know the accelerator available - NOT USED as we have switched to lightning\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some libraries that need to be installed\n",
    "! pip install torchviz graphviz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the source and target languages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source and target languages\n",
    "TARGET = 'tam'\n",
    "SOURCE = 'eng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n",
      "22\n",
      "20\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "# load all the available data and print sample counts for each set\n",
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 26\n",
      "Source Vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Source Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n",
      "Target Vocabulary Size = 46\n",
      "Target Vocabulary = ['ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்']\n",
      "Target Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'ஃ', 5: 'அ', 6: 'ஆ', 7: 'இ', 8: 'ஈ', 9: 'உ', 10: 'ஊ', 11: 'எ', 12: 'ஏ', 13: 'ஐ', 14: 'ஒ', 15: 'ஓ', 16: 'க', 17: 'ங', 18: 'ச', 19: 'ஜ', 20: 'ஞ', 21: 'ட', 22: 'ண', 23: 'த', 24: 'ந', 25: 'ன', 26: 'ப', 27: 'ம', 28: 'ய', 29: 'ர', 30: 'ற', 31: 'ல', 32: 'ள', 33: 'ழ', 34: 'வ', 35: 'ஷ', 36: 'ஸ', 37: 'ஹ', 38: 'ா', 39: 'ி', 40: 'ீ', 41: 'ு', 42: 'ூ', 43: 'ெ', 44: 'ே', 45: 'ை', 46: 'ொ', 47: 'ோ', 48: 'ௌ', 49: '்'}\n"
     ]
    }
   ],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "SRC_LANG = Language(SOURCE)\n",
    "TAR_LANG = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using train data only\n",
    "SRC_LANG.create_vocabulary(*(x_train))\n",
    "TAR_LANG.create_vocabulary(*(y_train))\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "SRC_LANG.generate_mappings()\n",
    "TAR_LANG.generate_mappings()\n",
    "\n",
    "# print the source and target vocabularies\n",
    "print(f'Source Vocabulary Size = {len(SRC_LANG.symbols)}')\n",
    "print(f'Source Vocabulary = {SRC_LANG.symbols}')\n",
    "print(f'Source Mapping {SRC_LANG.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(TAR_LANG.symbols)}')\n",
    "print(f'Target Vocabulary = {TAR_LANG.symbols}')\n",
    "print(f'Target Mapping {TAR_LANG.index2sym}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(plt.LightningModule):\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language, common_embed_size, common_num_layers, \n",
    "                 common_hidden_size, common_cell_type, enc_bidirect=False, attention=False, dropout=0.0, \n",
    "                 opt_name='Adam', learning_rate=1e-3, batch_size=32):\n",
    "    \n",
    "        super(Runner,self).__init__()\n",
    "        # save the language objects\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "\n",
    "        # create all the sub-networks and the main model\n",
    "        self.encoder = EncoderNet(vocab_size=src_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, bidirect=enc_bidirect, dropout=dropout)\n",
    "        if attention:\n",
    "            self.attention = True\n",
    "            self.attn_layer = Attention(common_hidden_size, enc_bidirect)\n",
    "        else:\n",
    "            self.attention = False\n",
    "            self.attn_layer = None\n",
    "        \n",
    "        self.decoder = DecoderNet(vocab_size=tar_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, attention=attention, attn_layer=self.attn_layer,\n",
    "                             enc_bidirect=enc_bidirect, dropout=dropout)\n",
    "        \n",
    "        self.model = EncoderDecoder(encoder=self.encoder, decoder=self.decoder, src_lang=src_lang, \n",
    "                                    tar_lang=tar_lang)\n",
    "\n",
    "        # for determinism\n",
    "        torch.manual_seed(42); torch.cuda.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "        self.model.apply(self.init_weights) # initialize model weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # optimizer for the model and loss function [that ignores locs where target = PAD token]\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(ignore_index=tar_lang.sym2index[PAD_SYM])\n",
    "        self.opt_name = opt_name\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # only adam is present in configure_optimizers as of now\n",
    "        if (opt_name != 'Adam'):\n",
    "            exit(-1)\n",
    "        \n",
    "        self.pred_train_words = []; self.true_train_words = []\n",
    "        self.pred_valid_words = []; self.true_valid_words = []\n",
    "        self.test_X_words = []; self.pred_test_words = []; self.true_test_words = []\n",
    "        self.save_test_preds = False\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "        if self.opt_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        '''\n",
    "        function to initialize the weights of the model parameters\n",
    "        '''\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                 nn.init.uniform_(param.data, -0.04, 0.04)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_accuracy(pred_words, tar_words):\n",
    "        ''' \n",
    "        compute the accuracy using (predicted words, target words) and return it.\n",
    "        exact word matching is used.\n",
    "        '''\n",
    "        assert(len(pred_words) == len(tar_words))\n",
    "        count = 0\n",
    "        for i in range(len(pred_words)):\n",
    "            if pred_words[i] == tar_words[i]:\n",
    "                count += 1\n",
    "        return count / len(pred_words)\n",
    "    \n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # load all the available data on all GPUs\n",
    "        self.x_train, self.y_train = load_data(TARGET, 'train')\n",
    "        self.x_valid, self.y_valid = load_data(TARGET, 'valid')\n",
    "        self.x_test, self.y_test = load_data(TARGET, 'test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_train, self.y_train, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_valid, self.y_valid, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_test, self.y_test, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        # we do inference word by word. So, batch_size = 1\n",
    "        return dataloader\n",
    "\n",
    "    ####################\n",
    "    # INTERFACE RELATED FUNCTIONS - NOTE -> early stopping; heatmap; beam decoding (and save top 3 preds);\n",
    "    #                                       plot test errors (for final); fix teacher forcing time interval;\n",
    "    #                                       wandb sweeping stuff and model checkpointing;\n",
    "    ####################\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = train_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens, tf_ratio=0.8)\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        # for epoch-level metrics[accuracy], log all the required data\n",
    "        self.true_train_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.pred_train_words += self.tar_lang.convert_to_words(preds)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.log('train_acc', 100*self.exact_accuracy(self.pred_train_words, self.true_train_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.pred_train_words.clear(); self.true_train_words.clear()\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = valid_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens) # no teacher forcing\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        # for epoch-level metrics[accuracy], log all the required data\n",
    "        self.true_valid_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.pred_valid_words += self.tar_lang.convert_to_words(preds)\n",
    "        self.log('valid_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log('valid_acc', 100*self.exact_accuracy(self.true_valid_words, self.pred_valid_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        self.true_valid_words.clear(); self.pred_valid_words.clear()\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = test_batch\n",
    "        logits, pred_word = self.model.greedy_inference(batch_X, X_lens)\n",
    "        self.pred_test_words += pred_word\n",
    "        self.true_test_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.test_X_words += self.src_lang.convert_to_words(batch_X)\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[1:, :]\n",
    "        # we shrink the logits to the true decoded sequence length for loss computation alone\n",
    "        true_dec_len = targets.size(1)\n",
    "        logits = (logits[:true_dec_len, :]).swapaxes(0,1).unsqueeze(0)\n",
    "        # squeeze and swapping of dimensions is to meet condition needed by nn.CrossEntopyLoss()\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def track_test_predictions(self):\n",
    "        self.save_test_preds = True\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log('test_acc', 100*self.exact_accuracy(self.pred_test_words, self.true_test_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        if not self.save_test_preds:\n",
    "            self.pred_test_words.clear(); self.true_test_words.clear(); self.test_X_words.clear()\n",
    "    \n",
    "    def save_test_predictions(self, fname='pred'):\n",
    "\n",
    "        edit_distances = [edit_dist(pred,tar) for pred, tar in zip(self.pred_test_words,self.true_test_words)]\n",
    "        pred_df = pd.DataFrame(zip(self.test_X_words, self.true_test_words, self.pred_test_words, edit_distances),\n",
    "                               columns=['Input', 'Target', 'Predicted', 'Edit Distance'])\n",
    "        \n",
    "        pred_df.to_csv('./'+fname+'.csv', index=False, encoding='utf-8')\n",
    "        \n",
    "        self.save_test_preds = False\n",
    "        # clear after saving to save memory \n",
    "        self.pred_test_words.clear(); self.true_test_words.clear(); self.test_X_words.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | encoder        | EncoderNet       | 4.3 M \n",
      "1 | decoder        | DecoderNet       | 1.5 M \n",
      "2 | model          | EncoderDecoder   | 5.8 M \n",
      "3 | loss_criterion | CrossEntropyLoss | 0     \n",
      "----------------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.238    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005685329437255859,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d674ecba2a4452faf35ecccf22622e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikram/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/vikram/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008150815963745117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7231af00ba4c22a83ec8257db73921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/vikram/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009472846984863281,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Testing",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d358393611c04186a787bd97ac419d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.4439518451690674     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.4439518451690674    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing runner\n",
    "# send tf_ratio (hparam) for 10 epochs (min_epochs); then turn on early stopping to track val_loss/val_acc\n",
    "# BEST CONFIG - NOTE (attn = True, 12 epochs, tf_ratio=0.8)\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 128, 3, 256, 'LSTM', True, False, 0.0, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "trainer = plt.Trainer(max_steps=5)\n",
    "trainer.fit(runner)\n",
    "runner.freeze()\n",
    "runner.track_test_predictions()\n",
    "trainer.test(runner)\n",
    "runner.save_test_predictions('temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE -> performing BUG SEARCH\n",
    "# testing all combinations to catch bugs\n",
    "# RESULT -> all clear; no bugs caught\n",
    "num_lay = [1,3]\n",
    "ctype = ['LSTM', 'GRU', 'RNN']\n",
    "bidirect = [True, False]\n",
    "attn = [True, False]\n",
    "\n",
    "for n in num_lay:\n",
    "    for c in ctype:\n",
    "        for b in bidirect:\n",
    "            for a in attn:\n",
    "                runner = Runner(SRC_LANG, TAR_LANG, 128, n, 256, c, b, a, 0.05, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "                trainer = plt.Trainer(max_steps=50)\n",
    "                trainer.fit(runner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
