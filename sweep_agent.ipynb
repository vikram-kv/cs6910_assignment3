{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Code - Code executed by a sweep agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for the notebook\n",
    "import lightning as lt\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "import wandb\n",
    "from language import *\n",
    "from dataset_dataloader import *\n",
    "from encoder_decoder import *\n",
    "from runner import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# know the accelerator available - NOT USED as we have switched to lightning\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the source and target languages; loading data to create Language objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source and target languages\n",
    "TARGET = 'tam'\n",
    "SOURCE = 'eng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the available data and print sample counts for each set\n",
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "SRC_LANG = Language(SOURCE)\n",
    "TAR_LANG = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using train data only\n",
    "SRC_LANG.create_vocabulary(*(x_train))\n",
    "TAR_LANG.create_vocabulary(*(y_train))\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "SRC_LANG.generate_mappings()\n",
    "TAR_LANG.generate_mappings()\n",
    "\n",
    "# print the source and target vocabularies\n",
    "print(f'Source Vocabulary Size = {len(SRC_LANG.symbols)}')\n",
    "print(f'Source Vocabulary = {SRC_LANG.symbols}')\n",
    "print(f'Source Mapping {SRC_LANG.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(TAR_LANG.symbols)}')\n",
    "print(f'Target Vocabulary = {TAR_LANG.symbols}')\n",
    "print(f'Target Mapping {TAR_LANG.index2sym}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sweep agent function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# Code executed by a sweep agent\n",
    "def agent_code():\n",
    "    wdbrun = wandb.init(project='cs6910-assignment3', entity='cs19b021')\n",
    "    wconfig = wandb.config\n",
    "    # rename run to identify config easily from it\n",
    "    wdbrun.name = f'ereemb={wconfig[\"embedding_size\"]}_layers={wconfig[\"number_of_layers\"]}_hid={wconfig[\"hidden_size\"]}'\n",
    "    wdbrun.name += f'_cell={wconfig[\"cell\"]}_bidirectional={wconfig[\"bidirectional\"]}_dr={wconfig[\"dropout\"]}'\n",
    "    wdbrun.name += f'_itfr={wconfig[\"initial_tf_ratio\"]}_bsize={wconfig[\"batch_size\"]}_att={wconfig[\"attention\"]}'\n",
    "    wdbrun.name += f'_opt={wconfig[\"optimizer\"]}_lr={wconfig[\"learning_rate\"]}'\n",
    "    # dictionary to pass to a model (instance of Runner Class)\n",
    "    rdict = dict(\n",
    "                SOURCE=SOURCE,\n",
    "                TARGET=TARGET,\n",
    "                src_lang=SRC_LANG,\n",
    "                tar_lang=TAR_LANG,\n",
    "                common_embed_size=wconfig['embedding_size'],\n",
    "                common_num_layers=wconfig['number_of_layers'],\n",
    "                common_hidden_size=wconfig['hidden_size'],\n",
    "                common_cell_type=wconfig['cell'],\n",
    "                init_tf_ratio= wconfig['initial_tf_ratio'],\n",
    "                enc_bidirect=wconfig['bidirectional'],\n",
    "                attention=wconfig['attention'],\n",
    "                dropout=wconfig['dropout'],\n",
    "                opt_name=wconfig['optimizer'],\n",
    "                learning_rate=wconfig['learning_rate'],\n",
    "                batch_size=wconfig['batch_size'] \n",
    "    )\n",
    "    \n",
    "    runner = Runner(**rdict)\n",
    "    # early stop if val_acc does not improve by 0.001 = 0.1% for 5 epochs\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_acc\", min_delta=wconfig['min_delta_imp'], patience=wconfig['patience'], verbose=True, mode=\"max\")\n",
    "    # we checkpoint the model when val_acc improves in the working directory.\n",
    "    chkCallback = ModelCheckpoint(dirpath='./', filename=f'{wdbrun.name}', monitor='val_acc', mode='max')\n",
    "    trainer = lt.Trainer(min_epochs=wconfig['min_epochs'], max_epochs=wconfig['max_epochs'], callbacks=[chkCallback, early_stop_callback])\n",
    "    trainer.fit(runner)\n",
    "    # log the checkpoint on wandb so that we can test later by loading it directly\n",
    "    artifact = wandb.Artifact(f'{wandb.run.name}_best_ckpt'.replace(\"=\",\"-\"), type='model')\n",
    "    artifact.add_file(chkCallback.best_model_path)\n",
    "    wandb.run.log_artifact(artifact)\n",
    "    # finish run\n",
    "    wdbrun.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start runs using configs from the sweep server at `sweep-id`; count=10 is set to avoid crossing kaggle usage limits\n",
    "wandb.agent(sweep_id='sweep-id', project='cs6910-assignment3', entity='cs19b021', count=10, function=agent_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
