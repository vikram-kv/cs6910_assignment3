{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment 3 (RNN Frameworks for transliteration) - Without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for the notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# set the device to 'cuda' if available\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# define the source and target languages\n",
    "TARGET = 'hin'\n",
    "SOURCE = 'eng'\n",
    "# define the special tokens that stand for start of seq, end of seq, \n",
    "# an unknown symbol.\n",
    "SOS_SYM = '@'\n",
    "EOS_SYM = '$'\n",
    "UNK_SYM = '!'\n",
    "# define a special token for padding - this helps with batch processing \n",
    "PAD_SYM = '%'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the 'cat' (= train/val/test) data of language 'lang'\n",
    "def load_data(lang, cat):\n",
    "    fcontents = open(f'aksharantar_sampled/{lang}/{lang}_{cat}.csv','r', encoding='utf-8').readlines()\n",
    "    pairs = [tuple(l.strip().split(',')) for l in fcontents]\n",
    "    x_data, y_data = list(map(list,zip(*pairs)))\n",
    "    return x_data, y_data\n",
    "\n",
    "# class for a language with useful functions.\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.lname = name\n",
    "    \n",
    "    # function to create the vocabulary(set of tokens) using the words in 'data'\n",
    "    # here, a token is either a special token or a lang character\n",
    "    def create_vocabulary(self, *data):\n",
    "        symbols = set()\n",
    "        for wd in data:\n",
    "            for c in wd:\n",
    "                symbols.add(c)\n",
    "        self.symbols = symbols\n",
    "    \n",
    "    # function to generate the index2sym (a number to a token) and \n",
    "    # sym2index (a token to a number) mappings using the vocabulary\n",
    "    def generate_mappings(self):\n",
    "        self.index2sym = {0: SOS_SYM, 1 : EOS_SYM, 2 : UNK_SYM, 3 : PAD_SYM}\n",
    "        self.sym2index = {SOS_SYM : 0, EOS_SYM : 1, UNK_SYM : 2, PAD_SYM : 3}\n",
    "        self.symbols = list(self.symbols)\n",
    "        self.symbols.sort()\n",
    "\n",
    "        for i, sym in enumerate(self.symbols):\n",
    "            self.sym2index[sym] = i + 4\n",
    "            self.index2sym[i+4] = sym\n",
    "        \n",
    "        self.num_tokens = len(self.index2sym.keys())\n",
    "    \n",
    "    # function to tokenize a word and convert all the tokens to\n",
    "    # their corr. numbers using sym2index\n",
    "    def convert_to_numbers(self, word):\n",
    "        enc = [self.sym2index[SOS_SYM]]\n",
    "        for ch in word:\n",
    "            if ch in self.sym2index.keys():\n",
    "                enc.append(self.sym2index[ch])\n",
    "            else:\n",
    "                enc.append(self.sym2index[UNK_SYM])\n",
    "        enc.append(self.sym2index[EOS_SYM])\n",
    "        return enc\n",
    "    \n",
    "    # convert a list of predictions (each prediction is a list of numbers)\n",
    "    # to the corresponding list of words using index2sym\n",
    "    # pred should be numpy array of shape (number_of_words, max_word_length)\n",
    "    # tokens after EOS_SYM are discarded\n",
    "    def convert_to_words(self, preds):\n",
    "        num = preds.shape[0]\n",
    "        words = [] \n",
    "        for i in range(num):\n",
    "            wd = ''\n",
    "            for idx in preds[i][1:]:\n",
    "                ch = self.index2sym[idx]\n",
    "                if ch != EOS_SYM:\n",
    "                    wd += ch\n",
    "            words.append(wd)\n",
    "        return words\n",
    "\n",
    "    # get the number assigned to a token\n",
    "    def get_index(self, sym):\n",
    "        return self.sym2index[sym]\n",
    "    \n",
    "    # get the number of tokens in the vocabulary\n",
    "    def get_size(self):\n",
    "        return self.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n"
     ]
    }
   ],
   "source": [
    "# load all the available data and print sample counts for each set\n",
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 26\n",
      "Source Vocabulary = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Source Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}\n",
      "Target Vocabulary Size = 64\n",
      "Target Vocabulary = ['ँ', 'ं', 'ः', 'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ऑ', 'ओ', 'औ', 'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', 'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'ळ', 'व', 'श', 'ष', 'स', 'ह', '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्']\n",
      "Target Mapping {0: '@', 1: '$', 2: '!', 3: '%', 4: 'ँ', 5: 'ं', 6: 'ः', 7: 'अ', 8: 'आ', 9: 'इ', 10: 'ई', 11: 'उ', 12: 'ऊ', 13: 'ऋ', 14: 'ए', 15: 'ऐ', 16: 'ऑ', 17: 'ओ', 18: 'औ', 19: 'क', 20: 'ख', 21: 'ग', 22: 'घ', 23: 'ङ', 24: 'च', 25: 'छ', 26: 'ज', 27: 'झ', 28: 'ञ', 29: 'ट', 30: 'ठ', 31: 'ड', 32: 'ढ', 33: 'ण', 34: 'त', 35: 'थ', 36: 'द', 37: 'ध', 38: 'न', 39: 'प', 40: 'फ', 41: 'ब', 42: 'भ', 43: 'म', 44: 'य', 45: 'र', 46: 'ल', 47: 'ळ', 48: 'व', 49: 'श', 50: 'ष', 51: 'स', 52: 'ह', 53: '़', 54: 'ऽ', 55: 'ा', 56: 'ि', 57: 'ी', 58: 'ु', 59: 'ू', 60: 'ृ', 61: 'ॅ', 62: 'े', 63: 'ै', 64: 'ॉ', 65: 'ो', 66: 'ौ', 67: '्'}\n"
     ]
    }
   ],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "SRC_LANG = Language(SOURCE)\n",
    "TAR_LANG = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using train data only\n",
    "SRC_LANG.create_vocabulary(*(x_train))\n",
    "TAR_LANG.create_vocabulary(*(y_train))\n",
    "\n",
    "# otherwise, use unicode characters (assigned codepoints) in the script's range\n",
    "# src_lang.create_vocabulary_range()\n",
    "# tar_lang.create_vocabulary_range()\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "SRC_LANG.generate_mappings()\n",
    "TAR_LANG.generate_mappings()\n",
    "\n",
    "# print the source and target vocabularies\n",
    "print(f'Source Vocabulary Size = {len(SRC_LANG.symbols)}')\n",
    "print(f'Source Vocabulary = {SRC_LANG.symbols}')\n",
    "print(f'Source Mapping {SRC_LANG.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(TAR_LANG.symbols)}')\n",
    "print(f'Target Vocabulary = {TAR_LANG.symbols}')\n",
    "print(f'Target Mapping {TAR_LANG.index2sym}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterateDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data, src_lang : Language, tar_lang : Language):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_enc, y_enc = self.x_data[idx], self.y_data[idx]\n",
    "        x_enc = self.src_lang.convert_to_numbers(x_enc)\n",
    "        y_enc = self.tar_lang.convert_to_numbers(y_enc) \n",
    "        return torch.tensor(x_enc, dtype=int), torch.tensor(y_enc,dtype=int), self.y_data[idx]\n",
    "\n",
    "class CollationFunction:\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language):\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        # sorting is to save encoder computation. \n",
    "        # reasoning : https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "        batch.sort(key = lambda x : len(x[0]), reverse=True)\n",
    "        src, tar, tar_words = zip(*batch)\n",
    "        src_lens = torch.tensor([len(x) for x in src], dtype=int)\n",
    "        src = nn.utils.rnn.pad_sequence(list(src), batch_first=True, padding_value=self.src_lang.get_index(PAD_SYM))\n",
    "        tar = nn.utils.rnn.pad_sequence(list(tar), batch_first=True, padding_value=self.tar_lang.get_index(PAD_SYM))\n",
    "        return src, tar, src_lens, tar_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, hid_size, cell_type, \n",
    "                 bidirect=False, dropout=0):\n",
    "        super(EncoderNet, self).__init__()\n",
    "        self.hidden_size = hid_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # we create the required architecture using the received parameters\n",
    "        kwargs = {'input_size':embed_size, 'hidden_size':hid_size, 'num_layers':num_layers, \n",
    "                 'bidirectional':bidirect, 'batch_first':True}\n",
    "        if num_layers > 1:\n",
    "            kwargs['dropout'] = dropout\n",
    "        if cell_type == 'RNN':\n",
    "            self.network = nn.RNN(**kwargs)\n",
    "        elif cell_type == 'LSTM':\n",
    "            self.network = nn.LSTM(**kwargs)\n",
    "        else:\n",
    "            self.network = nn.GRU(**kwargs)\n",
    "        \n",
    "        self.cell_type = cell_type\n",
    "        self.bidirect = bidirect\n",
    "\n",
    "        # for combining the final layer's forward and reverse directions' final hidden state\n",
    "        if (self.bidirect):\n",
    "            self.combine_forward_backward = nn.Linear(2 * hid_size, hid_size)\n",
    "\n",
    "    def forward(self, batch_x, batch_lens):\n",
    "        batch_x = self.embedding(batch_x)\n",
    "        batch_x = self.dropout(batch_x)\n",
    "        packed_batch_x = nn.utils.rnn.pack_padded_sequence(batch_x, lengths=batch_lens, batch_first=True, \n",
    "                                                           enforce_sorted=True)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            packed_outputs, (hidden_outputs, _) = self.network(packed_batch_x,)\n",
    "        else:\n",
    "            packed_outputs, hidden_outputs = self.network(packed_batch_x)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        if self.bidirect:\n",
    "            # remember 2nd dim in hidden_outputs; so, we have to concatenate forward and backward\n",
    "            # final hidden states along **1st dimension**\n",
    "            concat_hidden_state = torch.cat((hidden_outputs[-2,:,:], hidden_outputs[-1,:,:]), dim=1)\n",
    "            hidden_state = self.combine_forward_backward(concat_hidden_state)\n",
    "            hidden_state = torch.tanh(hidden_state)\n",
    "        else:\n",
    "            hidden_state = hidden_outputs[-1, :, :]\n",
    "        # hidden_state = (batch_size, hid_size); outputs = (batch_size, max_seq_len_batch, D * hid_size)\n",
    "        # d = 2 if bidirectional; else d = 1\n",
    "        return outputs, hidden_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, hid_size, cell_type, \n",
    "                 dropout=0):\n",
    "        super(DecoderNet, self).__init__()\n",
    "        self.hidden_size = hid_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "       # we create the required architecture using the received parameters\n",
    "        kwargs = {'input_size':embed_size, 'hidden_size':hid_size, 'num_layers':num_layers, \n",
    "                 'batch_first':True}\n",
    "        if num_layers > 1:\n",
    "            kwargs['dropout'] = dropout\n",
    "        if cell_type == 'RNN':\n",
    "            self.network = nn.RNN(**kwargs)\n",
    "        elif cell_type == 'LSTM':\n",
    "            self.network = nn.LSTM(**kwargs)\n",
    "        else:\n",
    "            self.network = nn.GRU(**kwargs)\n",
    "\n",
    "        self.cell_type = cell_type\n",
    "        self.out_layer = nn.Linear(hid_size, vocab_size)\n",
    "\n",
    "    # will always go 1 step forward in time (seqlen = L = 1)\n",
    "    # previous decoder state shape = [num_layers, batch_size, hid_size]\n",
    "    def forward(self, batch_y, prev_decoder_state):\n",
    "        batch_y = batch_y.unsqueeze(1) # batch_size is first dim\n",
    "        batch_y = self.embedding(batch_y)\n",
    "        batch_y = self.dropout(batch_y)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            decoder_hidden_state, decoder_cell_state = prev_decoder_state\n",
    "            outputs, (decoder_hidden_state, decoder_cell_state) = self.network(batch_y, (decoder_hidden_state, decoder_cell_state))\n",
    "        else:\n",
    "            outputs, decoder_hidden_state = self.network(batch_y, prev_decoder_state)\n",
    "        \n",
    "        outputs = outputs.squeeze(1) # remove seqlen dimension\n",
    "        logits = self.out_layer(outputs)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            return logits, (decoder_hidden_state, decoder_cell_state)\n",
    "        else:\n",
    "            return logits, decoder_hidden_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq(Encoder-Decoder) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore changing teacher forcing ratio to something epoch-based as sir suggested\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder :EncoderNet, decoder : DecoderNet, src_lang : Language, tar_lang : Language) -> None:\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.enc_model = encoder\n",
    "        self.dec_model = decoder\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "        self.cell_type = self.dec_model.cell_type\n",
    "    \n",
    "    def forward(self, batch_X, batch_Y, X_lens, tf_ratio=None):\n",
    "        batch_size = batch_X.size(0)\n",
    "        _, final_enc_hidden_state = self.enc_model(batch_X, X_lens)\n",
    "        num_dec_layers = self.dec_model.num_layers\n",
    "        decoder_state = torch.stack([final_enc_hidden_state for _ in range(num_dec_layers)], dim=0)\n",
    "        # we will feed the encoder output into each decoder layer's initial hidden state\n",
    "        # hidden_dec = (num_layers, batch_size, hid_size)\n",
    "\n",
    "        tarlength = batch_Y.size(1)\n",
    "        outlogits = torch.zeros(batch_size, tarlength, self.dec_model.vocab_size).to(device)\n",
    "        preds = torch.zeros(batch_size, tarlength).to(device)\n",
    "        dec_input = batch_Y[:,0]\n",
    "\n",
    "        if (self.cell_type == 'LSTM'):\n",
    "            init_dec_cell_state = torch.stack([torch.zeros_like(final_enc_hidden_state) for _ in range(num_dec_layers)], dim=0).to(device)\n",
    "            decoder_state = (decoder_state, init_dec_cell_state)\n",
    "        \n",
    "        for tstep in range(1, tarlength):\n",
    "            curlogits, decoder_state = self.dec_model(dec_input, decoder_state)\n",
    "            tf_force_input = batch_Y[:, tstep]\n",
    "            pred = torch.argmax(curlogits, dim=1).to(device)\n",
    "            dec_input = pred\n",
    "            if tf_ratio != None:\n",
    "                rand_num = torch.randn(1)[0]\n",
    "                if rand_num <= tf_ratio:\n",
    "                    dec_input = tf_force_input\n",
    "            outlogits[:, tstep, :] = curlogits \n",
    "            preds[:, tstep] = pred\n",
    "\n",
    "        return outlogits, preds\n",
    "\n",
    "    def inference_forward(self, batch_X, X_lens, max_dec_length):\n",
    "        batch_size = batch_X.shape[0]\n",
    "        pred_words = [None for _ in range(batch_size)]\n",
    "\n",
    "        _, final_enc_hidden_state = self.enc_model(batch_X, X_lens)\n",
    "        num_dec_layers = self.dec_model.num_layers\n",
    "        decoder_state = torch.stack([final_enc_hidden_state for _ in range(num_dec_layers)], dim=0)\n",
    "\n",
    "        outlogits = torch.zeros(batch_size, max_dec_length, self.dec_model.vocab_size).to(device)\n",
    "        preds = torch.zeros(batch_size, max_dec_length).to(device)\n",
    "        dec_input = torch.tensor([self.tar_lang.sym2index[SOS_SYM] for _ in range(batch_size)], device=device)\n",
    "\n",
    "        if (self.cell_type == 'LSTM'):\n",
    "            init_dec_cell_state = torch.stack([torch.zeros_like(final_enc_hidden_state) for _ in range(num_dec_layers)], dim=0).to(device)\n",
    "            decoder_state = (decoder_state, init_dec_cell_state)\n",
    "    \n",
    "        for tstep in range(1, max_dec_length):\n",
    "            curlogits, decoder_state = self.dec_model(dec_input, decoder_state)\n",
    "            pred = torch.argmax(curlogits, dim=1).to(device)\n",
    "            dec_input = pred\n",
    "            outlogits[:, tstep, :] = curlogits \n",
    "            preds[:, tstep] = pred\n",
    "\n",
    "        pred_words = self.tar_lang.convert_to_words(preds.cpu().numpy())\n",
    "        return pred_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Evaluation/Inference Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language, common_embed_size, num_enc_layers, num_dec_layers, \n",
    "                 common_hidden_size, common_cell_type, enc_bidirect, dropout, opt_name='Adam',\n",
    "                 learning_rate=1e-3):\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "        # create all the sub-networks and the main model\n",
    "        self.encoder = EncoderNet(vocab_size=src_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=num_enc_layers, hid_size=common_hidden_size,\n",
    "                             cell_type= common_cell_type, bidirect=enc_bidirect, dropout=dropout)\n",
    "        self.decoder = DecoderNet(vocab_size=tar_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=num_dec_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, dropout=dropout)\n",
    "        self.model = EncoderDecoder(encoder=self.encoder, decoder=self.decoder, src_lang=src_lang, \n",
    "                                    tar_lang=tar_lang)\n",
    "        # move model to the torch device\n",
    "        self.model.to(device)\n",
    "        # for reproducibility - seed everything with 42\n",
    "        torch.manual_seed(42); torch.cuda.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "        self.model.apply(self.init_weights) # initialize model weights\n",
    "        self.trainLoader, self.validLoader, self.testLoader = None, None, None\n",
    "        self.optimizer = None\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(ignore_index=tar_lang.sym2index[PAD_SYM])\n",
    "        if opt_name == 'Adam':\n",
    "            self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.05)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    def generate_data_loaders(self, data_X, data_y, batch_size):\n",
    "        dataset = TransliterateDataset(data_X, data_y, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def make_all_loaders(self, train_data, valid_data, test_data, batch_size):\n",
    "        train_X, train_y = train_data\n",
    "        valid_X, valid_y = valid_data\n",
    "        test_X, test_y = test_data\n",
    "\n",
    "        self.trainLoader = self.generate_data_loaders(train_X, train_y, batch_size)\n",
    "        self.validLoader = self.generate_data_loaders(valid_X, valid_y, batch_size)\n",
    "        self.testLoader = self.generate_data_loaders(test_X, test_y, batch_size)\n",
    "\n",
    "    def get_accuracy(self, pred_words, tar_words):\n",
    "        assert(len(pred_words) == len(tar_words))\n",
    "        count = 0\n",
    "        for i in range(len(pred_words)):\n",
    "            if pred_words[i] == tar_words[i]:\n",
    "                count += 1\n",
    "        return count / len(pred_words)\n",
    "\n",
    "    def train(self, epoch_number, tf_ratio=0.6):\n",
    "        assert(self.trainLoader != None); assert(self.optimizer != None)\n",
    "\n",
    "        # set model in training mode for autograd to be activated\n",
    "        self.model.train(); self.optimizer.zero_grad()\n",
    "        train_loss = 0.0\n",
    "        pred_words, true_words = [], []\n",
    "        with tqdm(self.trainLoader, unit='batch') as tqdmLoader:\n",
    "            for batch_X, batch_y, X_lens, y_words in tqdmLoader:\n",
    "                tqdmLoader.set_description(f'Epoch {epoch_number}')\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                # get the logits, preds for the current batch\n",
    "                logits, preds = self.model(batch_X, batch_y, X_lens, tf_ratio=tf_ratio)\n",
    "                # ignore loss for the first time step\n",
    "                targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "                logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "                loss = self.loss_criterion(logits, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step(); self.optimizer.zero_grad()\n",
    "                train_loss += loss.item()\n",
    "                batch_pred_words = self.tar_lang.convert_to_words(preds.cpu().numpy())\n",
    "                tqdmLoader.set_postfix(loss=loss.item())\n",
    "                true_words += y_words\n",
    "                pred_words += batch_pred_words\n",
    "        train_loss /= len(self.trainLoader)\n",
    "        train_acc = self.get_accuracy(pred_words, true_words)\n",
    "        print(f'Train Loss = {train_loss}; Train Accuracy = {train_acc * 100}')\n",
    "        return train_loss, train_acc, pred_words, true_words\n",
    "\n",
    "    def evaluate(self):\n",
    "        assert(self.validLoader != None)\n",
    "        self.model.eval()\n",
    "        valid_loss = 0.0\n",
    "        true_words, pred_words = [], []\n",
    "        with torch.no_grad():\n",
    "            with tqdm(self.validLoader, unit='batch') as tqdmLoader:\n",
    "                for batch_X, batch_y, X_lens, y_words in tqdmLoader:\n",
    "                    tqdmLoader.set_description(f'Validation')\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    # get the logits, preds for the current batch\n",
    "                    logits, preds = self.model(batch_X, batch_y, X_lens)\n",
    "                    # ignore loss for the first time step\n",
    "                    targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "                    logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "                    loss = self.loss_criterion(logits, targets)\n",
    "                    valid_loss += loss.item()\n",
    "                    batch_pred_words = self.tar_lang.convert_to_words(preds.cpu().numpy())\n",
    "                    tqdmLoader.set_postfix(loss=loss.item())\n",
    "                    true_words += y_words\n",
    "                    pred_words += batch_pred_words\n",
    "        valid_loss /= len(self.validLoader)\n",
    "        valid_acc = self.get_accuracy(pred_words, true_words)\n",
    "        print(f'Validation Loss = {valid_loss}; Validation Accuracy = {valid_acc * 100}')\n",
    "        return valid_loss, valid_acc, pred_words, true_words\n",
    "\n",
    "    def inference(self):\n",
    "        assert(self.testLoader != None)\n",
    "        self.model.eval()\n",
    "        pred_words, true_words = [], []\n",
    "        with torch.no_grad():\n",
    "            with tqdm(self.testLoader, unit='batch') as tqdmLoader:\n",
    "                for batch_X, _, X_lens, y_words in tqdmLoader:\n",
    "                    tqdmLoader.set_description(f'Testing')\n",
    "                    batch_X = batch_X.to(device)\n",
    "                    # get the predicted words for the current batch\n",
    "                    batch_pred_words = self.model.inference_forward(batch_X, X_lens, 30)\n",
    "                    true_words += y_words\n",
    "                    pred_words += batch_pred_words\n",
    "        test_acc = self.get_accuracy(pred_words, true_words)\n",
    "        print(f'Test Accuracy = {test_acc * 100}')\n",
    "        return pred_words, true_words\n",
    "    \n",
    "    def beam_search_inference(self, ):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing runner\n",
    "\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 128, 2, 2, 256, 'LSTM', True, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.make_all_loaders((x_train, y_train), (x_valid, y_valid), (x_test, y_test), 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 400/400 [00:23<00:00, 17.01batch/s, loss=0.108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07751706624403595; Train Accuracy = 40.044921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:01<00:00, 28.83batch/s, loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 2.0411860942840576; Validation Accuracy = 16.357421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 28.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.341796875\n",
      "देशभरममध्येीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह्् सर्वसंग्रह\n",
      "तुमच्यापैकीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 400/400 [00:23<00:00, 17.32batch/s, loss=0.0768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07374084708280862; Train Accuracy = 43.310546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:01<00:00, 29.84batch/s, loss=2.2] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 2.1188877150416374; Validation Accuracy = 15.869140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 27.51batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.146484375\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रहा सर्वसंग्रह\n",
      "तुमच्यापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 400/400 [00:22<00:00, 17.79batch/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07668202170170843; Train Accuracy = 43.255859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 34.27batch/s, loss=2.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 2.144007943570614; Validation Accuracy = 14.4775390625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 30.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.048828125\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 400/400 [00:22<00:00, 17.83batch/s, loss=0.0615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07598747181706131; Train Accuracy = 39.921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 38.67batch/s, loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 2.212528444826603; Validation Accuracy = 20.0439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 29.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.5126953125\n",
      "देशभरामध्येीीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपत अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचााओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गण्िविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 400/400 [00:22<00:00, 17.64batch/s, loss=0.0773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07681074621155858; Train Accuracy = 40.318359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 38.24batch/s, loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 2.243309684097767; Validation Accuracy = 17.3583984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 31.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.78125\n",
      "देशभरामध्येीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओंो लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रहा सर्वसंग्रह\n",
      "तुमच्यापैकीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 400/400 [00:22<00:00, 17.95batch/s, loss=0.102] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.07683271872811019; Train Accuracy = 34.7265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 38.53batch/s, loss=2.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 2.2410480678081512; Validation Accuracy = 12.79296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 28.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.244140625\n",
      "देशभरामध्येीई देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकाचनाओं लोकरचनाओं\n",
      "ट्राससप्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शकसनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रहााा सर्वसंग्रह\n",
      "तुमच्यापैकीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:  56%|█████▋    | 226/400 [00:13<00:10, 17.06batch/s, loss=0.0832]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m40\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     _, _, p, w \u001b[39m=\u001b[39m runner\u001b[39m.\u001b[39;49mtrain(i, \u001b[39m0.8\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     runner\u001b[39m.\u001b[39mevaluate(); runner\u001b[39m.\u001b[39minference()\n\u001b[1;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(p,w))[:\u001b[39m10\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[30], line 71\u001b[0m, in \u001b[0;36mRunner.train\u001b[0;34m(self, epoch_number, tf_ratio)\u001b[0m\n\u001b[1;32m     68\u001b[0m batch_X, batch_y \u001b[39m=\u001b[39m batch_X\u001b[39m.\u001b[39mto(device), batch_y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     70\u001b[0m \u001b[39m# get the logits, preds for the current batch\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m logits, preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch_X, batch_y, X_lens, tf_ratio\u001b[39m=\u001b[39;49mtf_ratio)\n\u001b[1;32m     72\u001b[0m \u001b[39m# ignore loss for the first time step\u001b[39;00m\n\u001b[1;32m     73\u001b[0m targets \u001b[39m=\u001b[39m batch_y[:, \u001b[39m1\u001b[39m:]; logits \u001b[39m=\u001b[39m logits[:, \u001b[39m1\u001b[39m:, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[43], line 29\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, batch_X, batch_Y, X_lens, tf_ratio)\u001b[0m\n\u001b[1;32m     26\u001b[0m     decoder_state \u001b[39m=\u001b[39m (decoder_state, init_dec_cell_state)\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m tstep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, tarlength):\n\u001b[0;32m---> 29\u001b[0m     curlogits, decoder_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec_model(dec_input, decoder_state)\n\u001b[1;32m     30\u001b[0m     tf_force_input \u001b[39m=\u001b[39m batch_Y[:, tstep]\n\u001b[1;32m     31\u001b[0m     pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(curlogits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mDecoderNet.forward\u001b[0;34m(self, batch_y, prev_decoder_state)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     34\u001b[0m     decoder_hidden_state, decoder_cell_state \u001b[39m=\u001b[39m prev_decoder_state\n\u001b[0;32m---> 35\u001b[0m     outputs, (decoder_hidden_state, decoder_cell_state) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(batch_y, (decoder_hidden_state, decoder_cell_state))\n\u001b[1;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     outputs, decoder_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(batch_y, prev_decoder_state)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:760\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m--> 760\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    761\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_weights_have_changed():\n\u001b[1;32m    762\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_flat_weights()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    _, _, p, w = runner.train(i, 0.8)\n",
    "    runner.evaluate(); runner.inference()\n",
    "    for x, y in list(zip(p,w))[:10]:\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 400/400 [00:22<00:00, 18.00batch/s, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.2406470539048314; Train Accuracy = 29.062500000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 32.14batch/s, loss=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.6984108313918114; Validation Accuracy = 12.6708984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 26.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 2.3193359375\n",
      "देशभरामध्येीू देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठीी् वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसि्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दलीीी शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमचियापैकीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 400/400 [00:22<00:00, 18.07batch/s, loss=0.161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.23019238682463766; Train Accuracy = 30.333984375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 32.05batch/s, loss=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.6871172301471233; Validation Accuracy = 16.7236328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 28.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 5.56640625\n",
      "देशभरामध्येीीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपतत अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेस ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमचायापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 400/400 [00:22<00:00, 17.71batch/s, loss=0.0989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.21431664464995265; Train Accuracy = 31.064453125000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 37.30batch/s, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.689727671444416; Validation Accuracy = 15.771484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 31.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 2.9052734375\n",
      "देशभरामध्ये देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओंी लोकरचनाओं\n",
      "ट्रांसक्प्रिट्स ट्रांसक्रिप्टेज़\n",
      "गणतिविधियों गंतिविधियों\n",
      "शासनाबद्दलआ शासनाबद्दल\n",
      "सर्वसंग्रहय् सर्वसंग्रह\n",
      "तुमच्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 400/400 [00:22<00:00, 18.01batch/s, loss=0.27] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.20776620188727976; Train Accuracy = 35.966796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 36.26batch/s, loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.6535076722502708; Validation Accuracy = 20.5322265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:00<00:00, 32.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 4.6875\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरानाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह् सर्वसंग्रह\n",
      "तुमच्यापैकीीई तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 400/400 [00:22<00:00, 17.85batch/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.19579787340015173; Train Accuracy = 35.314453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 34.98batch/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.6934340260922909; Validation Accuracy = 18.3349609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 30.80batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 5.078125\n",
      "देशभरामध्ये् देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठीीी वाहिन्यांसाठी\n",
      "लोकरचनाओंब लोकरचनाओं\n",
      "ट्रांस््रिप्टेज ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शशनाबद््दली शासनाबद्दल\n",
      "सर्वसंग्रहा सर्वसंग्रह\n",
      "तुमच्यापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 400/400 [00:22<00:00, 17.82batch/s, loss=0.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.18189316511154174; Train Accuracy = 35.728515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 38.25batch/s, loss=1.63]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.7000265531241894; Validation Accuracy = 17.28515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 24.17batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 5.6884765625\n",
      "देशभरामध्येीी देशभरामध्ये\n",
      "पेशनधारियोंं पेशनधारियों\n",
      "अनुक्रमानुपत अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यालिकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 400/400 [00:22<00:00, 17.69batch/s, loss=0.134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.1832525734603405; Train Accuracy = 34.068359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 36.61batch/s, loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.6814131401479244; Validation Accuracy = 14.94140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 28.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 1.171875\n",
      "देशभरामध्येीीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमणनुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमचियापैकीीईई तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 400/400 [00:22<00:00, 18.04batch/s, loss=0.168] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.17136392408981918; Train Accuracy = 34.681640625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 36.08batch/s, loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.7055077068507671; Validation Accuracy = 15.91796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 31.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 2.63671875\n",
      "देशभरामध्ये देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रम्नुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठीा वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह् सर्वसंग्रह\n",
      "तुमच्यापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 400/400 [00:22<00:00, 17.88batch/s, loss=0.0977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.1673356541618705; Train Accuracy = 31.15234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 35.25batch/s, loss=1.73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.7577137239277363; Validation Accuracy = 14.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 30.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 2.6123046875\n",
      "देशभरामध्येीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुसात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं्ो लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह् सर्वसंग्रह\n",
      "तुंच्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 400/400 [00:22<00:00, 17.87batch/s, loss=0.189] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.15876324309036136; Train Accuracy = 36.234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 36.11batch/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.7694206088781357; Validation Accuracy = 15.72265625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 30.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 3.0517578125\n",
      "देशभरांव्येीीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपास अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओंिि लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियें गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीीा तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 400/400 [00:22<00:00, 17.76batch/s, loss=0.138] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.15459850285202265; Train Accuracy = 36.251953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 34.75batch/s, loss=1.62]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.8071357160806656; Validation Accuracy = 17.1142578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 25.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 2.490234375\n",
      "देवभरामध्येीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह् सर्वसंग्रह\n",
      "तुमच्यापैकीा तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 400/400 [00:22<00:00, 18.05batch/s, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.14741685880348088; Train Accuracy = 38.92578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 35.59batch/s, loss=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.822924256324768; Validation Accuracy = 17.5537109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 29.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 4.296875\n",
      "देशभरामध्येीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपतत अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शकलाबदद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 400/400 [00:22<00:00, 17.85batch/s, loss=0.0933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.14510896801948547; Train Accuracy = 42.421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 35.80batch/s, loss=1.57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.8517569676041603; Validation Accuracy = 16.259765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 30.75batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 4.4921875\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरणनाओं लोकरचनाओं\n",
      "ट्राइसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसुग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 400/400 [00:22<00:00, 17.75batch/s, loss=0.0978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.15153653932735323; Train Accuracy = 39.521484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 35.10batch/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.7983729392290115; Validation Accuracy = 11.5966796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 31.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.9521484375\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रहानुपत अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गणतिविधियों गंतिविधियों\n",
      "शासनबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 400/400 [00:22<00:00, 17.80batch/s, loss=0.08]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.14508813774213195; Train Accuracy = 39.5234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 33.33batch/s, loss=1.71]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.8306889086961746; Validation Accuracy = 14.3310546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 24.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 1.513671875\n",
      "देशभरामध्येीा देशभरामध्ये\n",
      "पेषनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रहा सर्वसंग्रह\n",
      "तुमच्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 400/400 [00:22<00:00, 17.94batch/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.15186419202014803; Train Accuracy = 39.845703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 33.03batch/s, loss=1.84]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.8289672769606113; Validation Accuracy = 12.1337890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 25.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.439453125\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों् पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरणचायं लोकरचनाओं\n",
      "ट्राइब््रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियोंी गंतिविधियों\n",
      "शासनाबद्दलीी शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 400/400 [00:23<00:00, 17.11batch/s, loss=0.0886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.14001638181507586; Train Accuracy = 41.142578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 34.27batch/s, loss=1.65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.784283336251974; Validation Accuracy = 11.3037109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 23.41batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.1220703125\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानपपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह सर्वसंग्रह\n",
      "तुमच्यापैकीी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 400/400 [00:22<00:00, 17.67batch/s, loss=0.107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.13323773619718848; Train Accuracy = 44.34765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:01<00:00, 29.24batch/s, loss=1.69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.845846377313137; Validation Accuracy = 12.060546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 27.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.1953125\n",
      "देशभरामध्येीी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुुर्मननुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठीी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसब्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रह् सर्वसंग्रह\n",
      "तुमच्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 400/400 [00:22<00:00, 17.53batch/s, loss=0.149] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.13571722679771483; Train Accuracy = 39.939453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 34.52batch/s, loss=1.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.8471297174692154; Validation Accuracy = 8.642578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 29.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.0\n",
      "देशभरामध्येी देशभरामध्ये\n",
      "पेशनधारियों पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओं लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रही सर्वसंग्रह\n",
      "तुमच्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 400/400 [00:22<00:00, 17.69batch/s, loss=0.0545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 0.12466961739584803; Train Accuracy = 36.0703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 32/32 [00:00<00:00, 35.68batch/s, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 1.8715394958853722; Validation Accuracy = 12.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 32/32 [00:01<00:00, 29.63batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.2685546875\n",
      "देषभरामध्येी देशभरामध्ये\n",
      "पेशनधारियोंं पेशनधारियों\n",
      "अनुक्रमानुपात अनुक्रमानुपात\n",
      "वाहिन्यांसाठी वाहिन्यांसाठी\n",
      "लोकरचनाओंी लोकरचनाओं\n",
      "ट्रांसक्रिप्टेज़ ट्रांसक्रिप्टेज़\n",
      "गंतिविधियों गंतिविधियों\n",
      "शासनाबद्दल शासनाबद्दल\n",
      "सर्वसंग्रहण सर्वसंग्रह\n",
      "तुमछ्यापैकी तुमच्यापैकी\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    _, _, p, w = runner.train(i, 0.)\n",
    "    runner.evaluate(); runner.inference()\n",
    "    for x, y in list(zip(p,w))[:10]:\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
