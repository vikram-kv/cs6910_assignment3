{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS6910 Assignment 3 (RNN Frameworks for transliteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tqdm\n",
    "import wandb\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0834, -0.5957, -0.7612, -0.2037],\n",
      "        [ 0.0218, -0.9049, -0.3834,  1.2116],\n",
      "        [ 0.0391,  0.0981,  0.6162, -1.0046],\n",
      "        [-2.0050, -0.7412,  0.7707,  0.1779]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4,4)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0834, -0.5957],\n",
      "        [-0.7612, -0.2037],\n",
      "        [ 0.0218, -0.9049],\n",
      "        [-0.3834,  1.2116],\n",
      "        [ 0.0391,  0.0981],\n",
      "        [ 0.6162, -1.0046],\n",
      "        [-2.0050, -0.7412],\n",
      "        [ 0.7707,  0.1779]])\n",
      "tensor([[ 5.0000, -0.5957],\n",
      "        [-0.7612, -0.2037],\n",
      "        [ 0.0218, -0.9049],\n",
      "        [-0.3834,  1.2116],\n",
      "        [ 0.0391,  0.0981],\n",
      "        [ 0.6162, -1.0046],\n",
      "        [-2.0050, -0.7412],\n",
      "        [ 0.7707,  0.1779]])\n",
      "tensor([[ 5.0000, -0.5957, -0.7612, -0.2037],\n",
      "        [ 0.0218, -0.9049, -0.3834,  1.2116],\n",
      "        [ 0.0391,  0.0981,  0.6162, -1.0046],\n",
      "        [-2.0050, -0.7412,  0.7707,  0.1779]])\n"
     ]
    }
   ],
   "source": [
    "c = a.view(8,-1)\n",
    "print(c)\n",
    "c[0,0] = 5\n",
    "print(c)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "TARGET = 'tam'\n",
    "SOURCE = 'english'\n",
    "\n",
    "unicode_ranges = {'tam' : [0x0B80, 0x0BFF], \n",
    "                  'eng' : [0x0061, 0x007A],\n",
    "                  'hin' : [0x0900, 0x097F]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the 'cat' (= train/val/test) data of language 'lang'\n",
    "def load_data(lang, cat):\n",
    "    fcontents = open(f'aksharantar_sampled/{lang}/{lang}_{cat}.csv','r', encoding='utf-8').readlines()\n",
    "    pairs = [tuple(l.strip().split(',')) for l in fcontents]\n",
    "    x_data, y_data = list(map(list,zip(*pairs)))\n",
    "    return x_data, y_data\n",
    "\n",
    "# function to create the vocabulary using the words in 'data'\n",
    "def create_vocabulary(*data):\n",
    "    symbols = set()\n",
    "    for wd in data:\n",
    "        for c in wd:\n",
    "            symbols.add(c)\n",
    "    return symbols\n",
    "\n",
    "def create_vocabulary_range(lang):\n",
    "    symbols = set()\n",
    "    begin, end = unicode_ranges[lang]\n",
    "    for i in range(begin, end+1):\n",
    "        if (unicodedata.category(chr(i)) != 'Cn'):\n",
    "            symbols.add(chr(i))\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000, -1.4405, -0.3132, -0.5423,  1.0609],\n",
      "        [ 0.8650, -0.3950, -2.6440,  0.4028, -0.1971],\n",
      "        [ 1.8046,  0.0000,  0.2081,  0.0000, -0.0000],\n",
      "        [-0.1395, -0.7670,  1.5738,  2.5062,  0.0672],\n",
      "        [ 0.0000, -0.0000, -1.0513, -2.8951, -0.1279]]) tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487],\n",
      "        [ 0.6920, -0.3160, -2.1152,  0.3223, -0.1577],\n",
      "        [ 1.4437,  0.2660,  0.1665,  0.8744, -0.1435],\n",
      "        [-0.1116, -0.6136,  1.2590,  2.0050,  0.0537],\n",
      "        [ 0.6181, -0.4128, -0.8411, -2.3160, -0.1023]])\n",
      "tensor([[ 0.0000,  0.3245, -0.2175, -0.0000,  1.1728],\n",
      "        [ 0.6111, -0.8414,  1.0910, -1.5002,  1.6123],\n",
      "        [-1.8477,  0.0000, -0.5914,  0.4194,  1.8864],\n",
      "        [ 2.6024,  2.1334, -0.6246, -1.3337,  1.3937],\n",
      "        [-0.0000,  1.0072,  0.0000, -0.0000, -1.9989]]) tensor([[ 0.5627,  0.2596, -0.1740, -0.6787,  0.9383],\n",
      "        [ 0.4889, -0.6731,  0.8728, -1.2001,  1.2899],\n",
      "        [-1.4782,  2.5672, -0.4731,  0.3356,  1.5091],\n",
      "        [ 2.0820,  1.7067, -0.4997, -1.0670,  1.1149],\n",
      "        [-0.1407,  0.8058,  0.3276, -0.7607, -1.5991]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(5, 5)\n",
    "output = m(input)\n",
    "print(output, input)\n",
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(5, 5)\n",
    "output = m(input)\n",
    "print(output, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples = 51200\n",
      "Number of valid samples = 4096\n",
      "Number of test samples = 4096\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size = 72\n",
      "Source Vocabulary = {'ண', 'ா', 'அ', 'ொ', '௺', 'ஸ', '௬', 'ோ', 'ம', 'ௐ', '௭', 'ஔ', 'ீ', 'ஜ', 'ட', 'ஹ', 'ஶ', 'க', 'ன', 'வ', '௸', 'ெ', '௱', 'ை', '௩', 'ஷ', 'ப', 'ழ', 'ஒ', 'ௗ', 'ே', 'ி', 'ஞ', 'ஏ', 'ய', 'ற', 'எ', '௳', 'ஓ', '௹', '௴', '௲', 'ஐ', '௮', 'இ', 'ந', '௧', '௪', 'ஂ', 'த', '௯', '௷', '௶', 'உ', 'ு', 'ங', 'ள', '்', 'ௌ', 'ஆ', '௫', '௰', '௵', 'ர', 'ல', 'ூ', 'ச', 'ஃ', '௨', 'ஊ', '௦', 'ஈ'}\n"
     ]
    }
   ],
   "source": [
    "tam_vocab = create_vocabulary_range(TARGET)\n",
    "\n",
    "print(f'Source Vocabulary Size = {len(tam_vocab)}')\n",
    "print(f'Source Vocabulary = {tam_vocab}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
