{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweeping Code (for both attention and no attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries for the notebook\n",
    "import lightning as lt\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.functional import edit_distance as edit_dist\n",
    "import random\n",
    "import wandb\n",
    "from language import *\n",
    "from dataset_dataloader import *\n",
    "from encoder_decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# know the accelerator available - NOT USED as we have switched to lightning\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the source and target languages and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the source and target languages\n",
    "TARGET = 'tam'\n",
    "SOURCE = 'eng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the available data and print sample counts for each set\n",
    "x_train, y_train = load_data(TARGET, 'train')\n",
    "x_valid, y_valid = load_data(TARGET, 'valid')\n",
    "x_test, y_test = load_data(TARGET, 'test')\n",
    "\n",
    "print(f'Number of train samples = {len(x_train)}')\n",
    "print(f'Number of valid samples = {len(x_valid)}')\n",
    "print(f'Number of test samples = {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create language objects for storing vocabulary, index2sym and sym2index\n",
    "SRC_LANG = Language(SOURCE)\n",
    "TAR_LANG = Language(TARGET)\n",
    "\n",
    "# creating vocabulary using train data only\n",
    "SRC_LANG.create_vocabulary(*(x_train))\n",
    "TAR_LANG.create_vocabulary(*(y_train))\n",
    "\n",
    "# generate mappings from characters to numbers and vice versa\n",
    "SRC_LANG.generate_mappings()\n",
    "TAR_LANG.generate_mappings()\n",
    "\n",
    "# print the source and target vocabularies\n",
    "print(f'Source Vocabulary Size = {len(SRC_LANG.symbols)}')\n",
    "print(f'Source Vocabulary = {SRC_LANG.symbols}')\n",
    "print(f'Source Mapping {SRC_LANG.index2sym}')\n",
    "print(f'Target Vocabulary Size = {len(TAR_LANG.symbols)}')\n",
    "print(f'Target Vocabulary = {TAR_LANG.symbols}')\n",
    "print(f'Target Mapping {TAR_LANG.index2sym}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Pytorch lightning based module that encapsulates our seq2seq model with useful\n",
    "    helper functions\n",
    "'''\n",
    "class Runner(lt.LightningModule):\n",
    "    def __init__(self, src_lang : Language, tar_lang : Language, common_embed_size, common_num_layers, \n",
    "                 common_hidden_size, common_cell_type, init_tf_ratio = 0.8, enc_bidirect=False, attention=False, dropout=0.0, \n",
    "                 opt_name='Adam', learning_rate=2e-3, batch_size=32):\n",
    "    \n",
    "        super(Runner,self).__init__()\n",
    "        # save the language objects\n",
    "        self.src_lang = src_lang\n",
    "        self.tar_lang = tar_lang\n",
    "        # create all the sub-networks and the main model\n",
    "        self.encoder = EncoderNet(vocab_size=src_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, bidirect=enc_bidirect, dropout=dropout)\n",
    "        if attention:\n",
    "            self.attention = True\n",
    "            self.attn_layer = Attention(common_hidden_size, enc_bidirect)\n",
    "        else:\n",
    "            self.attention = False\n",
    "            self.attn_layer = None\n",
    "        \n",
    "        self.decoder = DecoderNet(vocab_size=tar_lang.get_size(), embed_size=common_embed_size,\n",
    "                             num_layers=common_num_layers, hid_size=common_hidden_size,\n",
    "                             cell_type=common_cell_type, attention=attention, attn_layer=self.attn_layer,\n",
    "                             enc_bidirect=enc_bidirect, dropout=dropout)\n",
    "        \n",
    "        self.model = EncoderDecoder(encoder=self.encoder, decoder=self.decoder, src_lang=src_lang, \n",
    "                                    tar_lang=tar_lang)\n",
    "\n",
    "        # for determinism\n",
    "        torch.manual_seed(42); torch.cuda.manual_seed(42); np.random.seed(42); random.seed(42)\n",
    "\n",
    "        self.model.apply(self.init_weights) # initialize model weights\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # optimizer for the model and loss function [that ignores locs where target = PAD token]\n",
    "        self.loss_criterion = nn.CrossEntropyLoss(ignore_index=tar_lang.sym2index[PAD_SYM])\n",
    "        self.opt_name = opt_name\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # only adam is present in configure_optimizers as of now\n",
    "        if (opt_name != 'Adam'):\n",
    "            exit(-1)\n",
    "\n",
    "        self.save_test_preds = False # true if we want to save test predictions and not clear them on test epoch end\n",
    "        self.cur_tf_ratio = init_tf_ratio # the current epoch teacher forcing ratio\n",
    "        self.min_tf_ratio = 0.01          # minimum allowed teacher forcing ratio\n",
    "\n",
    "        # lists for tracking predictions/true words etc...\n",
    "        self.pred_train_words = []; self.true_train_words = []\n",
    "        self.pred_valid_words = []; self.true_valid_words = []\n",
    "        self.test_X_words = []; self.pred_test_words = []; self.true_test_words = []\n",
    "        self.attn_matrices = []  # used only when there is attention layer\n",
    "\n",
    "        # lists for tracking losses\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "\n",
    "        # dictionary for logging at end of val epoch\n",
    "        self.wdb_logged_metrics = dict()\n",
    "        self.best_val_acc_seen = -0.01 # to save model weights on wandb\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = None\n",
    "        if self.opt_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        '''\n",
    "        function to initialize the weights of the model parameters\n",
    "        '''\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                 nn.init.uniform_(param.data, -0.04, 0.04)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_accuracy(pred_words, tar_words):\n",
    "        ''' \n",
    "        compute the accuracy using (predicted words, target words) and return it.\n",
    "        exact word matching is used.\n",
    "        '''\n",
    "        assert(len(pred_words) == len(tar_words))\n",
    "        count = 0\n",
    "        for i in range(len(pred_words)):\n",
    "            if pred_words[i] == tar_words[i]:\n",
    "                count += 1\n",
    "        return count / len(pred_words)\n",
    "    \n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # load all the available data on all GPUs\n",
    "        self.x_train, self.y_train = load_data(TARGET, 'train')\n",
    "        self.x_valid, self.y_valid = load_data(TARGET, 'valid')\n",
    "        self.x_test, self.y_test = load_data(TARGET, 'test')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_train, self.y_train, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_valid, self.y_valid, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=self.batch_size, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        return dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        dataset = TransliterateDataset(self.x_test, self.y_test, src_lang=SRC_LANG, tar_lang=TAR_LANG)\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=1, collate_fn=CollationFunction(SRC_LANG, TAR_LANG))\n",
    "        # we do inference word by word. So, batch_size = 1\n",
    "        return dataloader\n",
    "\n",
    "    ####################\n",
    "    # INTERFACE RELATED FUNCTIONS \n",
    "    ####################\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = train_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens, tf_ratio=self.cur_tf_ratio)\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        # for epoch-level metrics[accuracy], log all the required data\n",
    "        self.true_train_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.pred_train_words += self.tar_lang.convert_to_words(preds)\n",
    "        self.train_losses.append(loss) # to get train loss for epoch\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        # for wandb logging\n",
    "        self.wdb_logged_metrics['train_loss'] = torch.stack(self.train_losses).mean()\n",
    "        self.wdb_logged_metrics['train_acc'] = self.exact_accuracy(self.pred_train_words, self.true_train_words)\n",
    "        self.wdb_logged_metrics['tf_ratio'] = self.cur_tf_ratio\n",
    "        self.wdb_logged_metrics['epoch'] = self.current_epoch\n",
    "        self.train_losses.clear()\n",
    "\n",
    "        # note that on train_epoch_end is actually executed after valid epoch; so we log onto wandb here\n",
    "        if wandb.run is not None:\n",
    "            wandb.log(self.wdb_logged_metrics)\n",
    "\n",
    "        # for display bar\n",
    "        self.log('train_loss', self.wdb_logged_metrics['train_loss'], on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', self.wdb_logged_metrics['train_acc'], on_epoch=True, prog_bar=True)\n",
    "        self.pred_train_words.clear(); self.true_train_words.clear() # clear to save memory and for next epoch\n",
    "\n",
    "        # for first 12 epochs, we dont change the tf ratio. Then we decrease it by 0.1 every epoch till\n",
    "        # min_tf_ratio is reached. This is also logged.\n",
    "        if (self.current_epoch >= 11):\n",
    "            self.cur_tf_ratio -= 0.1\n",
    "            self.cur_tf_ratio = max(self.cur_tf_ratio, self.min_tf_ratio)\n",
    "\n",
    "    def validation_step(self, valid_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = valid_batch\n",
    "        # get the logits, preds for the current batch\n",
    "        logits, preds = self.model(batch_X, batch_y, X_lens) # no teacher forcing\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[:, 1:, :]\n",
    "        logits = logits.swapaxes(1, 2) # make class logits the second dimension as needed\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        # for epoch-level metrics[accuracy], log all the required data\n",
    "        self.true_valid_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.pred_valid_words += self.tar_lang.convert_to_words(preds)\n",
    "        self.valid_losses.append(loss) # to get val loss for epoch\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # for wandb logging\n",
    "        self.wdb_logged_metrics['val_loss'] = torch.stack(self.valid_losses).mean()\n",
    "        self.wdb_logged_metrics['val_acc'] = self.exact_accuracy(self.true_valid_words, self.pred_valid_words)\n",
    "        self.valid_losses.clear()\n",
    "        \n",
    "        # for display bar\n",
    "        self.log('val_loss', self.wdb_logged_metrics['val_loss'], on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.wdb_logged_metrics['val_acc'], on_epoch=True, prog_bar=True)\n",
    "\n",
    "        self.true_valid_words.clear(); self.pred_valid_words.clear() # clear to free memory and for next epoch\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        batch_X, batch_y, X_lens = test_batch\n",
    "        logits, pred_word, attn_matrix = self.model.greedy_inference(batch_X, X_lens)\n",
    "        # update all the global lists\n",
    "        self.pred_test_words += pred_word\n",
    "        self.true_test_words += self.tar_lang.convert_to_words(batch_y)\n",
    "        self.test_X_words += self.src_lang.convert_to_words(batch_X)\n",
    "        # if there is attention, update the attention list also\n",
    "        if (self.attention):\n",
    "            self.attn_matrices += [attn_matrix]\n",
    "        # ignore loss for the first time step\n",
    "        targets = batch_y[:, 1:]; logits = logits[1:, :]\n",
    "        # we shrink the logits to the true decoded sequence length for loss computation alone\n",
    "        true_dec_len = targets.size(1)\n",
    "        logits = (logits[:true_dec_len, :]).swapaxes(0,1).unsqueeze(0)\n",
    "        # squeeze and swapping of dimensions is to meet condition needed by nn.CrossEntopyLoss()\n",
    "        loss = self.loss_criterion(logits, targets)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "    \n",
    "    # will prevent clearing of global test lists on test epoch end\n",
    "    def track_test_predictions(self):\n",
    "        self.save_test_preds = True\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log('test_acc', self.exact_accuracy(self.pred_test_words, self.true_test_words), \n",
    "                 on_epoch=True, prog_bar=True)\n",
    "        if not self.save_test_preds:\n",
    "            self.pred_test_words.clear(); self.true_test_words.clear(); self.test_X_words.clear()\n",
    "            self.attn_matrices.clear()\n",
    "    \n",
    "    # here, we will save all the predictions made and also, return a copy of the list of attention\n",
    "    # matrices for generating heatmaps\n",
    "    def save_test_predictions(self, fname='pred'):\n",
    "        edit_distances = [edit_dist(pred,tar) for pred, tar in zip(self.pred_test_words,self.true_test_words)]\n",
    "        pred_df = pd.DataFrame(zip(self.test_X_words, self.true_test_words, self.pred_test_words, edit_distances),\n",
    "                               columns=['Input', 'Target', 'Predicted', 'Levenshtein Distance'])\n",
    "        pred_df.to_csv('./'+fname+'.csv', index=False, encoding='utf-8')\n",
    "\n",
    "        # if attention layer is present, we return attention matrices as well.\n",
    "        ret_info = None\n",
    "        if self.attention:\n",
    "            ret_info = (self.test_X_words.copy(), self.true_test_words.copy(), self.pred_test_words.copy(), self.attn_matrices.copy())\n",
    "        self.save_test_preds = False\n",
    "\n",
    "        # clear after saving to save memory \n",
    "        self.pred_test_words.clear(); self.true_test_words.clear(); self.test_X_words.clear()\n",
    "        self.attn_matrices.clear()\n",
    "        return ret_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing runner\n",
    "# BEST CONFIG - NOTE (attn = True, 12 epochs, tf_ratio=0.8)\n",
    "runner = Runner(SRC_LANG, TAR_LANG, 128, 3, 256, 'LSTM', 0.8, True, True, 0.0, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "trainer = lt.Trainer(max_epochs=12)\n",
    "\n",
    "trainer.fit(runner)\n",
    "runner.freeze()\n",
    "runner.track_test_predictions()\n",
    "trainer.test(runner)\n",
    "a, b, c, d = runner.save_test_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "sweep_configuration = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'no-attention-bayes',\n",
    "    'metric': {'goal': 'maximize', 'name': 'validation_accuracy'},\n",
    "    'parameters': {\n",
    "        'embedding_size' : {'values' : [16, 32, 64, 128, 192]},\n",
    "        'number_of_layers' : {'values' : [1, 2, 3]},\n",
    "        'hidden_size' : {'values' : [32, 64, 128, 192, 256]},\n",
    "        'cell' : {'values' : ['RNN', 'GRU', 'LSTM']},\n",
    "        'bidirectional' : {'values' : ['True', 'False']},\n",
    "        'dropout' : {'values' : [0.0, 0.05, 0.1, 0.2, 0.3]},\n",
    "        'initial_tf_ratio' : {'values' : [0.6, 0.7, 0.8, 0.9]},\n",
    "        'batch_size' : {'values' : [32, 64, 128]},\n",
    "        'attention' : {'value' : 'False'},\n",
    "        'dataset' : {'value' : 'aksharantar'},\n",
    "        'optimizer' : {'value' : 'Adam'},\n",
    "        'learning_rate' : {'value' : 2e-3},\n",
    "        'max_epochs' : {'value' : 35},\n",
    "        'patience' : {'value' : 5},\n",
    "    }\n",
    "}\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, entity='cs19b021', project='cs6910-assignment3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing runner\n",
    "# send tf_ratio (hparam) for 10 epochs (min_epochs); then turn on early stopping to track val_loss/val_acc\n",
    "# BEST CONFIG - NOTE (attn = True, 12 epochs, tf_ratio=0.8)\n",
    "\n",
    "wconfig = {\n",
    "        'embedding_size' : 128,\n",
    "        'number_of_layers' : 3,\n",
    "        'hidden_size' : 128,\n",
    "        'cell' : 'LSTM',\n",
    "        'bidirectional' : True,\n",
    "        'dropout' : 0.0,\n",
    "        'initial_tf_ratio' : 0.8,\n",
    "        'batch_size' : 128,\n",
    "        'attention' : True,\n",
    "        'dataset' : 'aksharantar',\n",
    "        'optimizer' : 'Adam',\n",
    "        'learning_rate' :  2e-3,\n",
    "        'max_epochs' : 1,\n",
    "        'patience' : 5,\n",
    "        'min_epochs': 12,\n",
    "        'min_delta_imp' : 0.001\n",
    "}\n",
    "\n",
    "def agent_code():\n",
    "    #wconfig = wandb.config\n",
    "    wdbrun = wandb.init(job_type='testing-code', project='cs6910-assignment3', entity='cs19b021', config=wconfig)\n",
    "    wdbrun.name = f'emb={wconfig[\"embedding_size\"]}_layers={wconfig[\"number_of_layers\"]}_hid={wconfig[\"hidden_size\"]}'\n",
    "    wdbrun.name += f'_cell={wconfig[\"cell\"]}_bidirectional={wconfig[\"bidirectional\"]}_dr={wconfig[\"dropout\"]}'\n",
    "    wdbrun.name += f'_itfr={wconfig[\"initial_tf_ratio\"]}_bsize={wconfig[\"batch_size\"]}_att={wconfig[\"attention\"]}'\n",
    "    wdbrun.name += f'_opt={wconfig[\"optimizer\"]}_lr={wconfig[\"learning_rate\"]}'\n",
    "    rdict = dict(\n",
    "                src_lang=SRC_LANG,\n",
    "                tar_lang=TAR_LANG,\n",
    "                common_embed_size=wconfig['embedding_size'],\n",
    "                common_num_layers=wconfig['number_of_layers'],\n",
    "                common_hidden_size=wconfig['hidden_size'],\n",
    "                common_cell_type=wconfig['cell'],\n",
    "                init_tf_ratio= wconfig['initial_tf_ratio'],\n",
    "                enc_bidirect=wconfig['bidirectional'],\n",
    "                attention=wconfig['attention'],\n",
    "                dropout=wconfig['dropout'],\n",
    "                opt_name=wconfig['optimizer'],\n",
    "                learning_rate=wconfig['learning_rate'],\n",
    "                batch_size=wconfig['batch_size'] \n",
    "        )\n",
    "    \n",
    "    runner = Runner(**rdict)\n",
    "    # early stop if val_acc does not improve by 0.001 = 0.1% for 5 epochs\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_acc\", min_delta=wconfig['min_delta_imp'], patience=wconfig['patience'], verbose=True, mode=\"max\")\n",
    "    chkCallback = ModelCheckpoint(dirpath='./', filename=f'{wdbrun.name}', monitor='val_acc', mode='max')\n",
    "    trainer = lt.Trainer(min_epochs=wconfig['min_epochs'], max_epochs=wconfig['max_epochs'], callbacks=[chkCallback, early_stop_callback])\n",
    "    trainer.fit(runner)\n",
    "    # log the checkpoint so that we can test by loading it directly\n",
    "    artifact = wandb.Artifact(f'{wandb.run.name}_best_ckpt'.replace(\"=\",\"-\"), type='model')\n",
    "    artifact.add_file(chkCallback.best_model_path)\n",
    "    wandb.run.log_artifact(artifact)\n",
    "    wdbrun.finish()\n",
    "\n",
    "agent_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(runner.state_dict, 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdict = dict(\n",
    "    src_lang=SRC_LANG,\n",
    "    tar_lang=TAR_LANG,\n",
    "    common_embed_size=wconfig['embedding_size'],\n",
    "    common_num_layers=wconfig['number_of_layers'],\n",
    "    common_hidden_size=wconfig['hidden_size'],\n",
    "    common_cell_type=wconfig['cell'],\n",
    "    init_tf_ratio= wconfig['initial_tf_ratio'],\n",
    "    enc_bidirect=wconfig['bidirectional'],\n",
    "    attention=wconfig['attention'],\n",
    "    dropout=wconfig['dropout'],\n",
    "    opt_name=wconfig['optimizer'],\n",
    "    learning_rate=wconfig['learning_rate'],\n",
    "    batch_size=wconfig['batch_size']\n",
    ")\n",
    "\n",
    "runner = Runner.load_from_checkpoint('./mythical-fighter-31_chkpt.ckt.ckpt', **rdict)\n",
    "runner.freeze()\n",
    "trainer.test(runner)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IGNORE SECTION -> PERFORMED BUG SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE -> performing BUG SEARCH\n",
    "# testing all combinations to catch bugs\n",
    "# RESULT -> all clear; no bugs caught\n",
    "num_lay = [1,3]\n",
    "ctype = ['LSTM', 'GRU', 'RNN']\n",
    "bidirect = [True, False]\n",
    "attn = [True, False]\n",
    "\n",
    "for n in num_lay:\n",
    "    for c in ctype:\n",
    "        for b in bidirect:\n",
    "            for a in attn:\n",
    "                runner = Runner(SRC_LANG, TAR_LANG, 128, n, 256, c, 0.8, b, a, 0.05, 'Adam', learning_rate=2e-3, batch_size=128)\n",
    "                trainer = lt.Trainer(max_steps=2)\n",
    "                trainer.fit(runner)\n",
    "                runner.freeze()\n",
    "                trainer.test(runner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
